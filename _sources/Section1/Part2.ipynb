{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Patient similarity networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Patient Similarity Network Construction**\n",
    "\n",
    "11. **DNA Methylation Network Analysis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# scientific and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from astropy.stats import median_absolute_deviation\n",
    "import mygene\n",
    "import astropy\n",
    "\n",
    "# graph and network libraries\n",
    "import networkx as nx\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the same expression matrix we can create a patient similarity network.\n",
    "- Transposing the matrix will switch the rows and columns,\n",
    "- meaning that patients will become the columns instead of genes\n",
    "- By doing this, you can compute the correlation (or similarity) between patients based on their gene expression profiles,\n",
    "- and then create a network where nodes represent patients and edges represent similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data directories for the project\n",
    "\n",
    "raw_data_dir = '/data/raw'\n",
    "intermediate_data_dir = '/data/intermediate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in os.path.join(raw_data_dir,\"expression_data_filtered.csv\")\n",
    "df_renamed = pd.read_csv(os.path.join(raw_data_dir,\"expression_data_filtered.csv\"),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the matrix\n",
    "patient_gene_matrix = df_renamed.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store different correlation matrices\n",
    "patient_correlation_matrices = {}\n",
    "\n",
    "# Pearson correlation\n",
    "patient_correlation_matrices['pearson'] = patient_gene_matrix.corr(method='pearson')\n",
    "\n",
    "# patient_correlation_matrices['biweight_midcorrelation'] = abs_bicorr(patient_gene_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_correlation(correlation_matrix, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Creates a graph from a correlation matrix using a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    correlation_matrix (pd.DataFrame): DataFrame containing the correlation matrix.\n",
    "    threshold (float): Threshold for including edges based on correlation value.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Graph created from the correlation matrix.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for node in correlation_matrix.columns:\n",
    "        G.add_node(node)\n",
    "\n",
    "    # Add edges with weights above the threshold\n",
    "    for i in range(correlation_matrix.shape[0]):\n",
    "        for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "            if i != j:  # Ignore the diagonal elements\n",
    "                weight = correlation_matrix.iloc[i, j]\n",
    "                if abs(weight) >= threshold:\n",
    "                    G.add_edge(correlation_matrix.index[i], correlation_matrix.columns[j], weight=weight)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def clean_graph(G, degree_threshold=1, keep_largest_component=True):\n",
    "    \"\"\"\n",
    "    Cleans the graph by performing several cleaning steps:\n",
    "    - Removes unconnected nodes (isolates)\n",
    "    - Removes self-loops\n",
    "    - Removes nodes with a degree below a specified threshold\n",
    "    - Keeps only the largest connected component (optional)\n",
    "\n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph to clean.\n",
    "    degree_threshold (int): Minimum degree for nodes to keep.\n",
    "    keep_largest_component (bool): Whether to keep only the largest connected component.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Cleaned graph.\n",
    "    \"\"\"\n",
    "    G = G.copy()  # Work on a copy of the graph to avoid modifying the original graph\n",
    "\n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Remove nodes with no edges (isolates)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    # Remove nodes with degree below the threshold\n",
    "    low_degree_nodes = [node for node, degree in dict(G.degree()).items() if degree < degree_threshold]\n",
    "    G.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "    # Keep only the largest connected component\n",
    "    if keep_largest_component:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    return G\n",
    "\n",
    "def print_graph_info(G):\n",
    "    \"\"\"\n",
    "    Prints basic information about the graph.\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(\"Sample nodes:\", list(G.nodes)[:10])  # Print first 10 nodes as a sample\n",
    "    print(\"Sample edges:\", list(G.edges(data=True))[:10])  # Print first 10 edges as a sample\n",
    "    \n",
    "    # Check for self-loops\n",
    "    self_loops = list(nx.selfloop_edges(G))\n",
    "    if self_loops:\n",
    "        print(f\"Number of self-loops: {len(self_loops)}\")\n",
    "        print(\"Self-loops:\", self_loops)\n",
    "    else:\n",
    "        print(\"No self-loops in the graph.\")\n",
    "\n",
    "def calculate_average_clustering(G):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average clustering coefficient of the graph.\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    print(f\"Average clustering coefficient: {avg_clustering}\")\n",
    "\n",
    "\n",
    "def knn_sparsification(graph, k):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by keeping only the top-k edges with the highest weights for each node.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    k (int): The number of nearest neighbors to keep for each node.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    \n",
    "    for node in graph_copy.nodes():\n",
    "        edges = sorted(graph_copy.edges(node, data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
    "        sparsified_graph.add_edges_from(edges[:k])\n",
    "    \n",
    "    return sparsified_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_pearson_graph = create_graph_from_correlation(patient_correlation_matrices['pearson'], threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(patient_pearson_graph, title='Pearson Correlation Network (Threshold = 0.8)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the graph by removing unconnected nodes\n",
    "patient_pearson_graph_pruned = clean_graph(patient_pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(patient_pearson_graph_pruned, title='Pearson Patient Correlation Network (Threshold = 0.8)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_pearson_graph_pruned_knn = knn_sparsification(patient_pearson_graph_pruned, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph_info(patient_pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph_info(patient_pearson_graph_pruned_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(patient_pearson_graph_pruned_knn, title='K-Nearest Neighbors (k=10) Patient Correlation Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(patient_pearson_graph_pruned_knn, os.path.join(intermediate_data_dir,'patient_coexpression_network_knn.gml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA methylation based Patient Similarity Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second task, we are preparing an additional network for the same patients, this time based on DNA methylation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using pickle\n",
    "with open(os.path.join(data_dir,\"ISMB_TCGA_DNAm.pkl\") , 'rb') as file : \n",
    "    data = pd.read_pickle(file)\n",
    "\n",
    "meth_data = data[\"datExpr\"]\n",
    "meth_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the pickle file\n",
    "with open(os.path.join(raw_data_dir,\"ISMB_TCGA_GE.pkl\"), 'rb') as file:\n",
    "    GE_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_data[\"datMeta\"][\"patient\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the smoking-related DNA methylation data from a tab-separated values (TSV) file. (os.path.join(raw_data_dir,\"smoking.tsv\"))\n",
    "2. Identify CpG sites that are commonly annotated in the smoking dataset\n",
    "3. Filter the DNA methylation data to include only the common CpG sites identified in the previous step\n",
    "4. Identify patients that are present in both the gene expression dataset and the methylation dataset.\n",
    "5. Filter the methylation data to include only the common patients and common CpG sites.\n",
    "6. Transpose the filtered methylation data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_df = pd.read_csv(os.path.join(raw_data_dir,\"smoking.tsv\") , delimiter='\\t') #Tab seperated document from EWAS\n",
    "common_annotated_cpgs = list(smoking_df['cpg'].value_counts()[smoking_df['cpg'].value_counts() > 10].index) \n",
    "cpgs = set(common_annotated_cpgs) & set(meth_data.columns)\n",
    "\n",
    "\n",
    "common_patients = set(GE_data[\"datMeta\"][\"patient\"].to_list()) & set(meth_data.index)\n",
    "\n",
    "\n",
    "meth_data_filt = meth_data.loc[ list(common_patients) , list(cpgs)]\n",
    "\n",
    "patient_meth_matrix = meth_data_filt.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_meth_matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pearson correlation of the data in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store different correlation matrices\n",
    "p_meth_correlation_matrices = {}\n",
    "\n",
    "# Pearson correlation\n",
    "p_meth_correlation_matrices['pearson'] = patient_meth_matrix.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_meth_correlation_matrices['pearson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_meth_pearson_graph = create_graph_from_correlation(p_meth_correlation_matrices['pearson'], threshold=0.8)\n",
    "# Clean the graph by removing unconnected nodes\n",
    "p_meth_pearson_graph_pruned = clean_graph(p_meth_pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=True)\n",
    "\n",
    "visualize_graph(p_meth_pearson_graph_pruned, title='Pearson Correlation Network (Threshold = 0.8)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_meth_pearson_graph_pruned_knn = knn_sparsification(p_meth_pearson_graph_pruned, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(p_meth_pearson_graph_pruned_knn, title='Pearson Correlation Network (Threshold = 0.8)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(p_meth_pearson_graph_pruned_knn, os.path.join(data_dir,'patient_meth_network_knn.gml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphs(correlation_matrices_dict, data_dir, matrix_names):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # Define parameter combinations\n",
    "    thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "    k_values = [3, 5, 7, 10]\n",
    "    degree_thresholds = [0, 1]\n",
    "    keep_largest_component_options = [False, True]\n",
    "\n",
    "    for matrix_name in matrix_names:\n",
    "        if matrix_name not in correlation_matrices_dict:\n",
    "            print(f\"Matrix {matrix_name} not found in the provided dictionary. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        correlation_matrix = correlation_matrices_dict[matrix_name]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Create the initial graph from the correlation matrix\n",
    "            initial_graph = create_graph_from_correlation(correlation_matrix, threshold=threshold)\n",
    "            \n",
    "            for degree_threshold in degree_thresholds:\n",
    "                for keep_largest_component in keep_largest_component_options:\n",
    "                    # Clean the graph\n",
    "                    cleaned_graph = clean_graph(initial_graph, degree_threshold=degree_threshold, keep_largest_component=keep_largest_component)\n",
    "                    \n",
    "                    for k in k_values:\n",
    "                        # Sparsify the graph using KNN\n",
    "                        sparsified_graph = knn_sparsification(cleaned_graph, k)\n",
    "                        \n",
    "                        # Calculate the average clustering coefficient\n",
    "                        avg_clustering = nx.average_clustering(sparsified_graph)\n",
    "                        \n",
    "                        # Construct the filename with relevant information\n",
    "                        filename = f\"{matrix_name}_network_threshold_{threshold}_k_{k}_degree_{degree_threshold}_largest_{keep_largest_component}_clustering_{avg_clustering:.2f}.gml\"\n",
    "                        filepath = os.path.join(data_dir, filename)\n",
    "                        \n",
    "                        # Save the graph to a GML file\n",
    "                        nx.write_gml(sparsified_graph, filepath)\n",
    "                        \n",
    "                        # Print the details\n",
    "                        print(f\"Saved: {filepath}\")\n",
    "\n",
    "def main(correlation_matrices_dict, matrix_names):\n",
    "    data_dir = '../../ismb_data/05072024/PSN-Met'  # Adjust the directory path as needed\n",
    "    save_graphs(correlation_matrices_dict, data_dir, matrix_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # correlation_matrices_dict = {\n",
    "    #     'pearson': pd.DataFrame(),  # Replace with actual data\n",
    "    #     'spearman': pd.DataFrame()  # Replace with actual data if needed\n",
    "    # }\n",
    "    matrix_names = ['pearson']  # Replace with the desired correlation matrix names\n",
    "    main(p_meth_correlation_matrices, matrix_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

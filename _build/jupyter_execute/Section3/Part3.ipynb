{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2886ee",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Part 3 : Smoking Network Classification using Graph Convolutional Networks</font> \n",
    "\n",
    "In this part we will work with a Graph Convolutional Network (GCN). \n",
    "\n",
    "In the previous part we formulated a task to predict whether individuals in the Generation Scotland Dataset have ever smoked. \n",
    "\n",
    "We generated a patient similarity network from CpG sites which are known to be associated with smoking. We will also work with the patient similarity network generated in section 1. \n",
    "\n",
    "In this part we postulate that individuals who have previously smoked will share similar epigenetic markers which we can learn from. \n",
    "We will now highlight the difference between our two networks to see if we can learn from them\n",
    "\n",
    "In this part we will be : \n",
    "- Working with Deep Graph Library\n",
    "- Building a GCN \n",
    "- Training a GCN\n",
    "- Comparing the performance of PSN's \n",
    "- Comparing the performance of GCN to simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75911ae1",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Import Functions and Packages </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0997595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0 , './../functions.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd1a4c",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Load Data and Network </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4106f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using pickle\n",
    "with open('./../data/GCN_Data.pkl' , 'rb') as file : \n",
    "    loaded_data = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c253f",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Working with [Deep Graph Library](https://www.dgl.ai/)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fc6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "g = dgl.from_networkx(loaded_data['PSN_EWAS'] , node_attrs=['idx' , 'label']) # Create a DGL graph from the networkx graph\n",
    "g.ndata['feat'] = torch.Tensor(loaded_data['Feat'].iloc[g.ndata['idx'].numpy()].values) # Add the features to the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b9df8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check that our graph is preserved\n",
    "phenotypes = loaded_data['phenotypes'] # Load the phenotypes\n",
    "\n",
    "node_0 = phenotypes.iloc[g.ndata['idx'].numpy()].iloc[0] # Get the idx of the first node of the dgl graph\n",
    "\n",
    "node_0_edges = []\n",
    "for e1 , e2 in loaded_data['PSN_EWAS'].edges(node_0.name) :  # Get the edges of this node in the networkx graph\n",
    "    node_0_edges.append(e2)\n",
    "        \n",
    "# Note : DGL will reorder the nodes if integers present in the node name\n",
    "\n",
    "sorted(list(phenotypes.iloc[g.ndata['idx'].numpy()].iloc[g.out_edges(0)[1].numpy()].index)) == sorted(node_0_edges) # Check if the edges match between the networkx and dgl graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cb887",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Graph Convolutional Network </font>\n",
    "\n",
    "Here we will specify the layers of the GCN model using the [GraphConv](https://docs.dgl.ai/en/2.0.x/generated/dgl.nn.pytorch.conv.GraphConv.html) class from DGL.\n",
    "        \n",
    "See [Object Oriented Programming](https://www.w3schools.com/python/python_classes.asp) for more information on building classes in python.\n",
    "\n",
    "See [GraphConv](https://docs.dgl.ai/en/2.0.x/generated/dgl.nn.pytorch.conv.GraphConv.html) for more documentation on the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a799e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim,  hidden_feats, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.gcnlayers = nn.ModuleList()\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        '''\n",
    "        #################################################\n",
    "                       YOUR CODE HERE\n",
    "        #################################################\n",
    "        The GraphConv class takes in the following arguments:\n",
    "            1. input_dim : The number of input features\n",
    "            2. output_dim : The number of output features\n",
    "            3. activation : The activation function to be used after the layer (we will stick to the default which is None)\n",
    "\n",
    "        We will create a list of GraphConv layers with the following dimensions:\n",
    "            1. input_dim -> hidden_feats[0]\n",
    "            2. hidden_feats[0] -> hidden_feats[1]\n",
    "            3. hidden_feats[1] -> num_classes\n",
    "\n",
    "        Where hidden_feats is a list of the number of hidden features in each layer which we will specify later\n",
    "        '''\n",
    "        self.gcnlayers.extend([\n",
    "            GraphConv(input_dim , hidden_feats[0]),\n",
    "            GraphConv(hidden_feats[0] , hidden_feats[1]),\n",
    "            GraphConv(hidden_feats[1] , num_classes)\n",
    "        ])        \n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including the input layer)\n",
    "        \n",
    "        '''\n",
    "        #################################################\n",
    "                       YOUR CODE HERE\n",
    "        #################################################\n",
    "        \n",
    "        Here we will define the forward pass of the GCN model. \n",
    "        \n",
    "        The forward pass will consist of the following steps:\n",
    "            1. Pass the input features through the first GraphConv layer\n",
    "            2. Apply dropout on the output of the first layer & ReLU activation function hint use F.relu\n",
    "            3. Pass the output of the first layer through the second GraphConv layer\n",
    "            4. Apply dropout on the output of the first layer & ReLU activation function hint use F.relu\n",
    "            5. Pass the output of the second layer through the third GraphConv layer\n",
    "            6. Return the output of the third layer\n",
    "        '''\n",
    "        \n",
    "        h = self.gcnlayers[0](g , h)\n",
    "        h = F.relu(self.drop(h))\n",
    "        h = self.gcnlayers[1](g , h)\n",
    "        h = F.relu(self.drop(h))\n",
    "        h = self.gcnlayers[2](g , h)\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba0721",
   "metadata": {},
   "source": [
    "### <font color='darkblue'> Training & Evaluation </font>\n",
    "\n",
    "We need to define training and evaluation functions for our GCN model. The code provided follows the outline defined in Chapter 6 of [Hamilton's book](https://www.cs.mcgill.ca/~wlh/grl_book/) for training GNN's. \n",
    "\n",
    "The code below is fully operational and provided for practical demonstration. Similar examples of GNN training functions are provided by DGL in their [Blitz Introduction](https://docs.dgl.ai/tutorials/blitz/index.html)\n",
    "\n",
    "In these functions we implement [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for multi-class classification. We use the [Adam Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) to update the model parameters and we use the [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) function to decay the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7ac6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve , average_precision_score , recall_score ,  PrecisionRecallDisplay\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(g, h, train_split , val_split , device ,  model , labels , epochs , lr):\n",
    "    # loss function, optimizer and scheduler\n",
    "    loss_fcn = nn.CrossEntropyLoss() # [Cross Entropy Loss for multi-class classification](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr , weight_decay=1e-4) # Optimizer to update model parameters [Adam Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8) # Scheduler function to decay the learning rate [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss   = []\n",
    "    train_acc  = []\n",
    "    val_acc    = []\n",
    "    \n",
    "    # training loop\n",
    "    epoch_progress = tqdm(total=epochs, desc='Loss : ', unit='epoch') # tqdm progress bar for epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        logits  = model(g, h) # Get the logits from the model. Logits are analagous to the node features estimates obtained during the karate club toy example\n",
    "\n",
    "        loss = loss_fcn(logits[train_split], labels[train_split].float()) # Calculate the loss i.e. how far off the model is from the true labels\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients before backpropagation\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Update the model parameters based on the gradients\n",
    "\n",
    "        scheduler.step() # Step the scheduler to decay the learning rate\n",
    "        \n",
    "        if (epoch % 5) == 0 :\n",
    "            \n",
    "            _, predicted = torch.max(logits[train_split], 1) # Get the predicted labels from the model\n",
    "            _, true = torch.max(labels[train_split] , 1) # Get the true labels\n",
    "            train_acc.append((predicted == true).float().mean().item()) # Calculate the accuracy of the model on the training set\n",
    "\n",
    "            valid_loss , valid_acc , *_ = evaluate(val_split, device, g , h, model , labels) # Evaluate the model on the validation set\n",
    "            val_loss.append(valid_loss.item())\n",
    "            val_acc.append(valid_acc)\n",
    "            \n",
    "            epoch_desc = (\n",
    "                \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                    epoch, np.mean(train_loss[-5:]) , np.mean(train_acc[-5:]), np.mean(val_acc[-5:])\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            epoch_progress.set_description(epoch_desc) # Update the progress bar description\n",
    "            epoch_progress.update(5) # Update the progress bar by 5 epochs\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    fig1 , ax1 = plt.subplots(figsize=(6,4))\n",
    "    ax1.plot(train_loss  , label = 'Train Loss')\n",
    "    ax1.plot(range(5 , len(train_loss)+1 , 5) , val_loss  , label = 'Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the training and validation accuracy\n",
    "    fig2 , ax2 = plt.subplots(figsize=(6,4))\n",
    "    ax2.plot(train_acc  , label = 'Train Accuracy')\n",
    "    ax2.plot(val_acc  , label = 'Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Close tqdm for epochs\n",
    "    epoch_progress.close()\n",
    "\n",
    "    return fig1 , fig2\n",
    "\n",
    "# Define a function to evaluate the model on the validation and test set \n",
    "def evaluate(split, device, g , h, model , labels):\n",
    "    model.eval() # Set the model to evaluation mode i.e. turn off dropout etc.. \n",
    "    loss_fcn = nn.CrossEntropyLoss() # Loss function for multi-class classification [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad() :  # Turn off gradient computation for evaluation\n",
    "        logits = model(g, h) # Get the logits from the model\n",
    "        \n",
    "        loss = loss_fcn(logits[split], labels[split].float()) # Calculate the loss on the validation or test set\n",
    "\n",
    "        _, predicted = torch.max(logits[split], 1) # Get the predicted labels\n",
    "        _, true = torch.max(labels[split] , 1) # Get the true labels\n",
    "        acc = (predicted == true).float().mean().item() # Calculate the accuracy of the model\n",
    "        \n",
    "        logits_out = logits[split].cpu().detach().numpy()  # Calculate the loss i.e. how far off the model is from the true labels\n",
    "        binary_out = (logits_out == logits_out.max(1).reshape(-1,1))*1 # Convert the logits to binary predictions\n",
    "        \n",
    "        labels_out = labels[split].cpu().detach().numpy() # Get the true labels\n",
    "        \n",
    "        PRC =  average_precision_score(labels_out , binary_out , average=\"weighted\") # Calculate the precision\n",
    "        SNS = recall_score(labels_out , binary_out , average=\"weighted\") # Calculate the recall\n",
    "        F1 = 2*((PRC*SNS)/(PRC+SNS)) # Calculate the F1 score\n",
    "        \n",
    "    \n",
    "    return loss , acc , F1 , PRC , SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a871ce31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=23676, out=128, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=128, out=32, normalization=both, activation=None)\n",
      "    (2): GraphConv(in=32, out=2, normalization=both, activation=None)\n",
      "  )\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1000, num_edges=39060,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int8), 'feat': Scheme(shape=(23676,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287c9742198a49328630a70e26d55b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loss :   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[0;32m---> 30\u001b[0m loss_plot \u001b[38;5;241m=\u001b[39m train(g, g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m] , train_index , val_index , device ,  model , labels , \u001b[38;5;241m1000\u001b[39m , \u001b[38;5;241m1e-3\u001b[39m) \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     33\u001b[0m test_output_metrics \u001b[38;5;241m=\u001b[39m evaluate(test_index , device , g , g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m] , model , labels ) \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g, h, train_split, val_split, device, model, labels, epochs, lr)\u001b[0m\n\u001b[1;32m     27\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero the gradients before backpropagation\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update the model parameters based on the gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Step the scheduler to decay the learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:215\u001b[0m, in \u001b[0;36mGSpMM.backward\u001b[0;34m(ctx, dZ)\u001b[0m\n\u001b[1;32m    213\u001b[0m         dX \u001b[38;5;241m=\u001b[39m gspmm(g_rev, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_lhs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, dZ, Y)\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_lhs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         dX \u001b[38;5;241m=\u001b[39m gspmm(g_rev, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_lhs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, dZ, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# max/min\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     dX \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    218\u001b[0m         (X_shape[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m dZ\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    219\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:1032\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m   1030\u001b[0m args \u001b[38;5;241m=\u001b[39m _cast_if_autocast_enabled(gidx, op, reduce_op, lhs_data, rhs_data)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _disable_autocast_if_enabled():\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GSpMM\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu') # Set the device to GPU if available else CPU\n",
    "\n",
    "g = g.to(device) # Move the graph to the device\n",
    "\n",
    "# Get the node labels from the meta data, order by network nodes and reset the index\n",
    "node_subjects = phenotypes['Smoking'].iloc[g.ndata['idx'].detach().cpu().numpy()].reset_index(drop=True) # Get node target labels from meta data \n",
    "node_subjects.name = 'Smoking'\n",
    "\n",
    "GCN_input_shapes = g.ndata['feat'].shape[1] # Get the input shape of the GCN model\n",
    "\n",
    "labels = F.one_hot(g.ndata['label'].to(torch.int64)) # Encode the labels as one hot vectors\n",
    "\n",
    "output_metrics = []\n",
    "logits = np.array([])\n",
    "labels_all = np.array([])\n",
    "\n",
    "train_tmp_index , test_index = train_test_split(                    # Split the data into a temporary training and test set\n",
    "    node_subjects.index, train_size=0.6, stratify=node_subjects\n",
    "    )\n",
    "train_index , val_index = train_test_split(                         # Split the temporary training set into a training and validation set\n",
    "    train_tmp_index, train_size=0.8, stratify=node_subjects.loc[train_tmp_index]\n",
    "    )\n",
    "\n",
    "model = GCN(GCN_input_shapes, hidden_feats=[128 , 32], num_classes=len(node_subjects.unique())).to(device) # Create the GCN model\n",
    "print(model)\n",
    "print(g)\n",
    "\n",
    "loss_plot = train(g, g.ndata['feat'] , train_index , val_index , device ,  model , labels , 1000 , 1e-3) # Train the model\n",
    "plt.show()\n",
    "\n",
    "test_output_metrics = evaluate(test_index , device , g , g.ndata['feat'] , model , labels ) # Evaluate the model on the test set\n",
    "\n",
    "# Print the test accuracy and F1 score\n",
    "print(\n",
    "    \"GNN Model | Test Accuracy = {:.4f} | F1 = {:.4f} |\".format(\n",
    "     test_output_metrics[1] , test_output_metrics[2] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38418729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression ACC : 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Barry\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # Import the logistic regression model from sklearn\n",
    "\n",
    "y_vals = labels.detach().cpu().numpy() # Get the labels as numpy arrays\n",
    "input_data = g.ndata['feat'].detach().cpu().numpy() # Get the input features as numpy arrays\n",
    "\n",
    "glmnet = LogisticRegression(penalty='l1' , solver='saga', max_iter = 1000 ) # Create a logistic regression model with L1 regularization\n",
    "glmnet.fit(input_data[train_index] , y_vals[train_index][: , 0]) # Fit the logistic regression model\n",
    "glmnet_acc = glmnet.score(input_data[test_index] , y_vals[test_index][: , 0]) # Get the accuracy of the logistic regression model on the test set\n",
    "print(f'Baseline Logistic Regression ACC : {glmnet_acc}') # Print the accuracy of the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0abeb",
   "metadata": {},
   "source": [
    "Our model does not outperform a simple linear regression model. This suggests that the gloabal relationships in the input data are stronger than the local relationships of our network\n",
    "\n",
    "i.e. The local structure enforced by our network is less informative that learning from all features at once\n",
    "\n",
    "In the next part, we will exhibit the predictive power of informative local structure by making a more informative network created from more than one modality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
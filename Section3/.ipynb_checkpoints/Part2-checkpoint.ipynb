{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2886ee",
   "metadata": {},
   "source": [
    "# <font color='darkblue'>Smoking Network Classification using Graph Convolutional Networks</font> \n",
    "\n",
    "## Section 4 - Machine Learning for Networks (lead Barry)\n",
    "- talk introducing ML methods specifically adapted for graphs from first principles\n",
    "- notebooks covering\n",
    " - implementation examples for a couple of GNN model types\n",
    " - an example analysis using one of the toy examples\n",
    " - a skeleton for analysis of the GenScot network\n",
    " \n",
    "<font color='darkgreen'> Idea so far : </font>\n",
    "\n",
    "1. From the [EWAS Catalog](https://www.ewascatalog.org/?trait=smoking) get a list of CpG sites and genes relating to smoking\n",
    "2. Subset to matching CpG sites in the methylation data\n",
    "3. Swap CpG sites to corresponding Gene (obtained from EWAS)\n",
    "4. Generate smoking \"disease\" PPI network from [StringDB](https://string-db.org)\n",
    "5. Build a data graph dataset which incorporates the Gen Scot methylation data\n",
    "6. Build a graph classifier which can predict Gen Scot participants who have or have not smoked\n",
    "\n",
    "<font color='red'> TO DO </font>\n",
    "1. Incorporate Seb's GNN's\n",
    "        1.1 Good to get Seb's help & input on this\n",
    "~~2. Faster baseline implementation -> Logistic Regression CV is too slow~~\n",
    "3. Improve comments and explain each section\n",
    "4. Mapping CpG's to genes is not a 1:1 mapping. Genes will be associated with multiple CpG's and vice versa. Need to identify a way better accounts for this\n",
    "        4.1 Currently aggregating by mean and dropping cpgs with multiple genes\n",
    "        4.2 Could add extra edges ? \n",
    "                4.2.1 Extra edge for two CpG's which are invovled in the same gene\n",
    "                4.2.3 Add nodes and edges between genes which are impacted by the same CpG\n",
    "5. Beat Logistic Regression Baseline\n",
    "\n",
    "<font color='Blue'> Updates </font> <br>\n",
    "Looks like network needs to be imrpoved. The average node degree is 3 which means that all the relationships are being smoothed out. We want a dense network with meaningful interactions. \n",
    "* Have tested with node features for 1 smoker 0 never and got 100% acc -> Model works fine\n",
    "* Have also test with logistic regression on the node features without a network ~78% accuracy. Small drop off from baseline but close enough\n",
    "* Began looking at a patient similarity framework too. Can measure similarity based off the smoking phenotypes\n",
    "    * Accuracy around 65% - improvement but not great\n",
    "    * potential to use gcn-mme from MOGDx? \n",
    "* Also playing around with looking at measuring similarity based off StringDB interactions **np.matmul(PatxGene , PPI)**\n",
    "    * Similar accuracy\n",
    "* This again is falling into the smoothing issue where the network has so few interactions that the scores become uniform. Likely what is happening with the graph classification too\n",
    "* I also added a for loop to the PPI StringDB whereby we now include all genes. The issue with this method is that it assumes genes in seperate for loops do not interact. Probably explains why the degree of the network is so low. \n",
    "* Next steps has to look at how we are developing the network and how to best generate this -> Stay with StringDB? \n",
    "    * Filtering Network definitely improves performance -> Increasing connectivity is important\n",
    "    * Might be better to cluster the large network and select biggest cluster?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20284d4",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Graph Convolutional Network in the Transductive Setting </font>\n",
    "\n",
    "In this part we will work with a graph convolutional network. We will attempt to predict whether individuals in the Generation Scotland Dataset have ever smoked. We will work with the network type - patient similarity network whereby we postulate that individuals who have previously smoked will share similar epigenetic markers.\n",
    "\n",
    "- Formulate the task\n",
    "- Patient Similarity Network Generation \n",
    "- Working with Deep Graph Library\n",
    "- Building a Graph Convolutional Network \n",
    "- Training a GCN \n",
    "- Limitations of the Transductive Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0997595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import astropy.stats\n",
    "from palettable import wesanderson\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd1a4c",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Task Formulation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064cad8",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Get a list of Smoking related CpG sites from the [EWAS Catalog](https://www.ewascatalog.org/)</font>\n",
    "\n",
    "In this case we are looking at the smoking phenotype in a cohort from Generation Scotland. We search for Smoking related CpG sites and download the related tab seperated file. We extract the CpG's and related genes which we will use to build our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cb4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('smoking.tsv' , delimiter='\\t') #Tab seperated document from EWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843eb654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>consortium</th>\n",
       "      <th>pmid</th>\n",
       "      <th>date</th>\n",
       "      <th>trait</th>\n",
       "      <th>efo</th>\n",
       "      <th>analysis</th>\n",
       "      <th>source</th>\n",
       "      <th>outcome</th>\n",
       "      <th>exposure</th>\n",
       "      <th>...</th>\n",
       "      <th>chrpos</th>\n",
       "      <th>chr</th>\n",
       "      <th>pos</th>\n",
       "      <th>gene</th>\n",
       "      <th>type</th>\n",
       "      <th>beta</th>\n",
       "      <th>se</th>\n",
       "      <th>p</th>\n",
       "      <th>details</th>\n",
       "      <th>study_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sikdar S</td>\n",
       "      <td>CHARGE</td>\n",
       "      <td>31536415</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>EFO_0009115, EFO_0006527, EFO_0004318</td>\n",
       "      <td>random effects meta-analysis</td>\n",
       "      <td>Table S2</td>\n",
       "      <td>DNA methylation</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>...</td>\n",
       "      <td>chr2:233284934</td>\n",
       "      <td>2</td>\n",
       "      <td>233284934</td>\n",
       "      <td>-</td>\n",
       "      <td>Island</td>\n",
       "      <td>-0.08545</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31536415_Sikdar-S_smoking_random_effects_meta-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sikdar S</td>\n",
       "      <td>CHARGE</td>\n",
       "      <td>31536415</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>EFO_0009115, EFO_0006527, EFO_0004318</td>\n",
       "      <td>random effects meta-analysis</td>\n",
       "      <td>Table S2</td>\n",
       "      <td>DNA methylation</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>...</td>\n",
       "      <td>chr19:17000585</td>\n",
       "      <td>19</td>\n",
       "      <td>17000585</td>\n",
       "      <td>F2RL3</td>\n",
       "      <td>North shore</td>\n",
       "      <td>-0.07233</td>\n",
       "      <td>0.00129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31536415_Sikdar-S_smoking_random_effects_meta-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sikdar S</td>\n",
       "      <td>CHARGE</td>\n",
       "      <td>31536415</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>EFO_0009115, EFO_0006527, EFO_0004318</td>\n",
       "      <td>random effects meta-analysis</td>\n",
       "      <td>Table S2</td>\n",
       "      <td>DNA methylation</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>...</td>\n",
       "      <td>chr5:373378</td>\n",
       "      <td>5</td>\n",
       "      <td>373378</td>\n",
       "      <td>AHRR</td>\n",
       "      <td>North shore</td>\n",
       "      <td>-0.03138</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31536415_Sikdar-S_smoking_random_effects_meta-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sikdar S</td>\n",
       "      <td>CHARGE</td>\n",
       "      <td>31536415</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>EFO_0009115, EFO_0006527, EFO_0004318</td>\n",
       "      <td>random effects meta-analysis</td>\n",
       "      <td>Table S2</td>\n",
       "      <td>DNA methylation</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>...</td>\n",
       "      <td>chr2:233284402</td>\n",
       "      <td>2</td>\n",
       "      <td>233284402</td>\n",
       "      <td>-</td>\n",
       "      <td>Island</td>\n",
       "      <td>-0.07381</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31536415_Sikdar-S_smoking_random_effects_meta-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sikdar S</td>\n",
       "      <td>CHARGE</td>\n",
       "      <td>31536415</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>EFO_0009115, EFO_0006527, EFO_0004318</td>\n",
       "      <td>random effects meta-analysis</td>\n",
       "      <td>Table S2</td>\n",
       "      <td>DNA methylation</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>...</td>\n",
       "      <td>chr6:30720080</td>\n",
       "      <td>6</td>\n",
       "      <td>30720080</td>\n",
       "      <td>-</td>\n",
       "      <td>Open sea</td>\n",
       "      <td>-0.08558</td>\n",
       "      <td>0.00159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31536415_Sikdar-S_smoking_random_effects_meta-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     author consortium      pmid        date    trait  \\\n",
       "0  Sikdar S     CHARGE  31536415  2019-09-19  Smoking   \n",
       "1  Sikdar S     CHARGE  31536415  2019-09-19  Smoking   \n",
       "2  Sikdar S     CHARGE  31536415  2019-09-19  Smoking   \n",
       "3  Sikdar S     CHARGE  31536415  2019-09-19  Smoking   \n",
       "4  Sikdar S     CHARGE  31536415  2019-09-19  Smoking   \n",
       "\n",
       "                                     efo                      analysis  \\\n",
       "0  EFO_0009115, EFO_0006527, EFO_0004318  random effects meta-analysis   \n",
       "1  EFO_0009115, EFO_0006527, EFO_0004318  random effects meta-analysis   \n",
       "2  EFO_0009115, EFO_0006527, EFO_0004318  random effects meta-analysis   \n",
       "3  EFO_0009115, EFO_0006527, EFO_0004318  random effects meta-analysis   \n",
       "4  EFO_0009115, EFO_0006527, EFO_0004318  random effects meta-analysis   \n",
       "\n",
       "     source          outcome exposure  ...          chrpos chr        pos  \\\n",
       "0  Table S2  DNA methylation  Smoking  ...  chr2:233284934   2  233284934   \n",
       "1  Table S2  DNA methylation  Smoking  ...  chr19:17000585  19   17000585   \n",
       "2  Table S2  DNA methylation  Smoking  ...     chr5:373378   5     373378   \n",
       "3  Table S2  DNA methylation  Smoking  ...  chr2:233284402   2  233284402   \n",
       "4  Table S2  DNA methylation  Smoking  ...   chr6:30720080   6   30720080   \n",
       "\n",
       "    gene         type     beta       se    p details  \\\n",
       "0      -       Island -0.08545  0.00131  0.0     NaN   \n",
       "1  F2RL3  North shore -0.07233  0.00129  0.0     NaN   \n",
       "2   AHRR  North shore -0.03138  0.00078  0.0     NaN   \n",
       "3      -       Island -0.07381  0.00132  0.0     NaN   \n",
       "4      -     Open sea -0.08558  0.00159  0.0     NaN   \n",
       "\n",
       "                                            study_id  \n",
       "0  31536415_Sikdar-S_smoking_random_effects_meta-...  \n",
       "1  31536415_Sikdar-S_smoking_random_effects_meta-...  \n",
       "2  31536415_Sikdar-S_smoking_random_effects_meta-...  \n",
       "3  31536415_Sikdar-S_smoking_random_effects_meta-...  \n",
       "4  31536415_Sikdar-S_smoking_random_effects_meta-...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c65dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df[['cpg' , 'gene']] #Subset to CpG and Genes\n",
    "df_filt = df_filt[df_filt['gene'] != '-'] #Remove CpG without annotated genes\n",
    "\n",
    "df_filt = df_filt.drop_duplicates(subset = ['cpg' , 'gene'])\n",
    "df_filt = df_filt.set_index('cpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2a67f",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Import and Process Generation Scotland Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ISMB_GS_Data.pkl' , 'rb') as file : \n",
    "    loaded_data = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51559786",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = loaded_data['DNAm_w1'].T.dropna() # we are only working with Wave 1 patients for this task\n",
    "phenotypes = loaded_data['Phenotypes'].set_index('Sample_SentrixID').loc[w1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71ab078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoking_cat(pack_years) : \n",
    "    if pack_years == 0 :\n",
    "        return 'Never'\n",
    "    else :\n",
    "        return 'Smoker'\n",
    "\n",
    "phenotypes['Smoking'] = phenotypes['pack_years'].apply(smoking_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff10cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency of graph training and learning we will downsample the dataset size to 1000 patients\n",
    "never_smoked = phenotypes[phenotypes['Smoking'] == 'Never'].sample(500)\n",
    "smoked = phenotypes[phenotypes['Smoking'] == 'Smoker'].sample(500)\n",
    "\n",
    "phenotypes = pd.concat([never_smoked , smoked])\n",
    "w1 = w1.loc[phenotypes.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d15d27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phenotypes['Smoking'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befee3a",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Patient Similarity Network Generation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b8c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_annotated_cpgs = list(df['cpg'].value_counts()[df['cpg'].value_counts() > 10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88f5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpgs = set(common_annotated_cpgs) & set(w1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af83ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_filt = w1.loc[ : , list(cpgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "754a874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_bicorr(data) : \n",
    "\n",
    "    data = data._get_numeric_data()\n",
    "    cols = data.columns\n",
    "    idx = cols.copy()\n",
    "    mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n",
    "    mat = mat.T\n",
    "\n",
    "    K = len(cols)\n",
    "    correl = np.empty((K, K), dtype=np.float32)\n",
    "    mask = np.isfinite(mat)\n",
    "\n",
    "    bicorr = astropy.stats.biweight_midcovariance(mat)\n",
    "\n",
    "    for i in range(K) : \n",
    "        correl[i , : ] = bicorr[i , :] / np.sqrt(bicorr[i,i] * np.diag(bicorr))\n",
    "        \n",
    "    return pd.DataFrame(data = correl , index=idx , columns=cols , dtype=np.float32)\n",
    "\n",
    "def get_k_neighbours(df , k ) : \n",
    "    k_neighbours = {}\n",
    "    if abs(df.max().max()) > 1 : \n",
    "        print('Dataframe should be a similarity matrix of max value 1')\n",
    "    else:\n",
    "        np.fill_diagonal(df.values , 1)\n",
    "        for node in df.index : \n",
    "            neighbours = df.loc[node].nlargest(k+1).index.to_list()[1:] #Exclude the node itself\n",
    "            k_neighbours[node] = neighbours\n",
    "        \n",
    "    return k_neighbours\n",
    "\n",
    "def plot_knn_network(df , K , labels , node_colours = ['skyblue'] , node_size = 300) : \n",
    "    # Get K-nearest neighbours for each node\n",
    "    k_neighbours = get_k_neighbours(df , k = K)\n",
    "    \n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    G.add_nodes_from(df.index)\n",
    "    \n",
    "    nx.set_node_attributes(G , labels.astype('category').cat.codes , 'label')\n",
    "    nx.set_node_attributes(G , pd.Series(np.arange(len(df.index)) , index=df.index) , 'idx')\n",
    "\n",
    "    # Add edges based on the k-nearest neighbours\n",
    "    for node, neighbours in k_neighbours.items():\n",
    "        for neighbor in neighbours:\n",
    "            G.add_edge(neighbor, node)\n",
    "\n",
    "    #plt.figure(figsize=(10, 8))\n",
    "    #nx.draw(G, with_labels=False, font_weight='bold', node_size=node_size, node_color=node_colours, font_size=8)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def gen_graph_legend(node_colours , G , attr) : \n",
    "    \n",
    "    patches = []\n",
    "    for col , lab in zip(node_colours.drop_duplicates() , pd.Series(nx.get_node_attributes(G , attr)).drop_duplicates()) : \n",
    "        patches.append(mpatches.Patch(color=col, label=lab))\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e598b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_colour = phenotypes['Smoking'].astype('category').cat.set_categories(wesanderson.FantasticFox2_5.hex_colors , rename=True)\n",
    "\n",
    "G = plot_knn_network(abs_bicorr(w1_filt.T) , 15 , phenotypes['Smoking'] , node_colours=node_colour)\n",
    "plt.title('Patient Smoking Similarity Network')\n",
    "legend_handles = gen_graph_legend(node_colour , G , 'label')\n",
    "plt.legend(handles = legend_handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae074a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('983812922632_R03C01', '565300924984_R08C01', '605479080718_R04C01', '962216799749_R07C01', '836497085616_R03C01', '546695233047_R02C01', '500079725023_R07C01', '518166592574_R02C01', '251141423630_R06C01', '188993848294_R05C01', '471697814409_R04C01', '941113024116_R07C01', '237003845025_R02C01', '530479725132_R08C01', '298487846709_R07C01', '780995603917_R06C01', '870033671633_R01C01', '183529994180_R03C01', '487666290134_R02C01', '598202879397_R03C01', '533725106249_R07C01', '338351716289_R04C01', '809128490680_R04C01', '141607582061_R01C01', '162572071974_R04C01', '404183884905_R08C01', '428101799801_R02C01', '812874808663_R07C01', '697768424200_R05C01', '498957085234_R07C01', '985333917364_R07C01', '819577203021_R06C01', '452243981511_R01C01', '443444850416_R08C01', '202332167680_R04C01', '298487846709_R05C01', '185988662299_R06C01', '316824574906_R08C01', '224527397604_R07C01', '283657642650_R07C01', '506462191560_R02C01', '294817686733_R02C01', '483852006866_R07C01', '852421958619_R05C01', '818977001509_R02C01', '778220460366_R01C01', '403648991923_R06C01', '181373123304_R04C01', '350574937750_R07C01', '346659466264_R08C01', '772507864220_R06C01', '415756034352_R07C01', '613678225918_R02C01', '297544515794_R08C01', '332567902971_R03C01', '314970302917_R02C01', '743388506713_R07C01', '804927994325_R07C01', '668705676811_R01C01', '176936739762_R05C01', '498957085234_R05C01', '614846268108_R04C01', '555693136394_R01C01', '803137604256_R04C01', '146322782850_R01C01', '434040027563_R01C01', '873521585701_R01C01', '259122074387_R06C01', '537936333862_R04C01', '600262957712_R01C01', '565567925919_R06C01', '596651647747_R07C01', '879277252141_R07C01', '394322724092_R02C01', '181373123304_R08C01', '590850938074_R08C01', '574978128031_R07C01', '484531821918_R04C01', '837336869336_R08C01', '550432335735_R04C01', '980561656392_R06C01', '404183884905_R02C01', '719632217236_R07C01', '232059887939_R08C01', '268974016901_R02C01', '443444850416_R06C01', '459195675061_R06C01', '141607582061_R05C01', '549638808810_R03C01', '500079725023_R03C01', '677304620474_R02C01', '326346266243_R08C01', '318699302621_R08C01', '787567883357_R07C01', '243280636096_R02C01', '350574937750_R01C01', '445944614274_R04C01', '846459856518_R04C01', '817436423280_R05C01', '102457222968_R05C01', '680938354872_R02C01', '350574937750_R05C01', '727247151228_R06C01', '689231219698_R01C01', '367445200026_R08C01', '959466257766_R08C01', '948783561055_R04C01', '302829486490_R08C01', '225328821563_R03C01', '790058703467_R01C01', '289429140398_R08C01', '335729275343_R07C01', '574925169248_R04C01', '466448743307_R01C01', '811546692775_R06C01', '846459856518_R08C01', '165527333350_R02C01', '409033458685_R03C01', '459812077549_R07C01', '446858017081_R01C01', '370843053704_R04C01', '626398765215_R01C01', '443444850416_R04C01', '640734353014_R06C01', '299345641390_R06C01', '193402818640_R08C01', '612705195389_R01C01', '614846268108_R03C01', '833388998512_R05C01', '294510884565_R01C01', '268974016901_R01C01', '154405239041_R05C01', '644510089362_R01C01', '394322724092_R03C01', '413109511303_R07C01', '415756034352_R08C01', '502445215324_R02C01', '675646703419_R08C01', '556110265353_R05C01', '807724809939_R02C01', '262668420369_R08C01', '329006498138_R04C01', '240378238690_R02C01', '574925169248_R06C01', '337407806513_R02C01', '185161767685_R03C01', '329006498138_R08C01', '985681824931_R01C01', '694667102821_R05C01', '506462191560_R06C01', '537600205152_R05C01', '506462191560_R05C01', '556782804890_R03C01', '878735920474_R02C01', '157657238789_R06C01', '801550375066_R07C01', '125611104983_R04C01', '417559122788_R03C01', '625347134215_R02C01', '676054142798_R04C01', '283657642650_R08C01', '425984903461_R03C01', '339942941850_R04C01', '667041731766_R01C01', '974942162848_R02C01', '717634550988_R08C01', '350574937750_R03C01', '565567925919_R08C01', '404183884905_R06C01', '509093861330_R08C01', '259499946600_R05C01', '655669055162_R06C01', '656285322665_R03C01', '694667102821_R06C01', '413379145326_R02C01', '152513362107_R08C01', '565458418029_R02C01', '970175547844_R01C01', '889976622096_R08C01', '237003845025_R03C01', '310732956604_R03C01', '545485318925_R05C01', '861950674936_R06C01', '718358656399_R08C01', '596651647747_R08C01', '712959680814_R03C01', '312434972765_R07C01', '941474856343_R01C01', '719632217236_R03C01', '268714929489_R02C01', '205700129657_R01C01', '421510609538_R07C01', '816223603163_R08C01', '675646703419_R06C01', '111027746808_R07C01', '237003845025_R01C01', '985681824931_R07C01', '675996819663_R06C01', '154091750079_R06C01', '780873329325_R07C01', '991694260988_R01C01', '444540536866_R06C01', '687358070409_R02C01', '302425809462_R01C01', '407902755147_R01C01', '240378238690_R07C01', '225328821563_R08C01', '940026495009_R01C01', '893056782702_R04C01', '720599104217_R03C01', '841210177280_R02C01', '930607649948_R05C01', '177869651878_R04C01', '338351716289_R07C01', '455309526127_R06C01', '497640694542_R01C01', '455309526127_R02C01', '424462208781_R05C01', '347131797225_R08C01', '807724809939_R06C01', '996217092342_R06C01', '625422120631_R02C01', '301748988408_R03C01', '259499946600_R07C01', '263592413038_R01C01', '897653450654_R01C01', '610041040103_R08C01', '849386891474_R03C01', '232086152780_R01C01', '869639170419_R01C01', '487666290134_R05C01', '174895720073_R03C01', '821372752060_R03C01', '259557760053_R06C01', '532662929054_R01C01', '633747641265_R08C01', '896152806391_R03C01', '681810253811_R08C01', '556110265353_R06C01', '194903682526_R04C01', '347131797225_R05C01', '371831823467_R07C01', '999353006252_R08C01', '589440363926_R03C01', '232059887939_R05C01', '489386889509_R05C01', '852421958619_R08C01', '847690575495_R07C01', '667041731766_R02C01', '162572071974_R08C01', '735324904897_R01C01', '407902755147_R08C01', '332567902971_R08C01', '612873860160_R05C01', '676054142798_R01C01', '726475683343_R08C01', '413109511303_R08C01', '676415833218_R03C01', '752748847082_R04C01', '590850938074_R04C01', '455309526127_R01C01', '891759613014_R03C01', '941474856343_R08C01', '556110265353_R08C01', '954171999232_R04C01', '567888327914_R04C01', '679966486485_R02C01', '833981914157_R06C01', '781769030593_R03C01', '948783561055_R07C01', '849386891474_R07C01', '617317247313_R05C01', '437233300655_R04C01', '744585980749_R02C01', '406038705185_R08C01', '720974616550_R05C01', '418500757729_R01C01', '735324904897_R08C01', '409033458685_R05C01', '532662929054_R08C01', '105389737102_R01C01', '596230551484_R08C01', '873548359327_R08C01', '423303532349_R08C01', '225443631984_R07C01', '459195675061_R07C01', '501674807024_R08C01', '675646703419_R03C01', '125611104983_R05C01', '379699511633_R06C01', '382975773895_R02C01', '620305099240_R03C01', '641691034411_R05C01', '204656393806_R07C01', '780873329325_R01C01', '682120615525_R07C01', '612873860160_R06C01', '791757627635_R07C01', '879277252141_R03C01', '431787841683_R02C01', '748394829067_R05C01', '943809127801_R05C01', '302829486490_R05C01', '694667102821_R01C01', '720974616550_R06C01', '152513362107_R04C01', '992665766587_R05C01', '185161767685_R02C01', '161538385077_R08C01', '177869651878_R07C01', '751983786989_R08C01', '772039750222_R08C01', '318699302621_R07C01', '500079725023_R04C01', '941474856343_R06C01', '101675284524_R05C01', '102457222968_R03C01', '625104810311_R04C01', '819577203021_R03C01', '552800327810_R08C01', '620121161198_R07C01', '511043123237_R01C01', '744585980749_R04C01', '999353006252_R05C01', '111027746808_R02C01', '335402317616_R01C01', '892662879977_R06C01', '326197298338_R05C01', '371831823467_R08C01', '202332167680_R08C01', '150054325625_R01C01', '869639170419_R03C01', '486970665126_R03C01', '558648734868_R01C01', '101675284524_R07C01', '176936739762_R06C01', '268714929489_R07C01', '185161767685_R05C01', '159642596676_R03C01', '419617709803_R03C01', '681810253811_R04C01', '873910789671_R08C01', '125611104983_R07C01', '444821662390_R07C01', '346821186406_R06C01', '732310311550_R08C01', '720887575043_R03C01', '849386891474_R04C01', '565458418029_R08C01', '264241292499_R08C01', '297447134307_R06C01', '124087179681_R01C01', '546695233047_R08C01', '175246310263_R06C01', '891759613014_R06C01', '892662879977_R08C01', '443444850416_R07C01', '878735920474_R01C01', '868489140011_R08C01', '679676522044_R08C01', '415756034352_R01C01', '811546692775_R03C01', '819577203021_R01C01', '465696867211_R06C01', '614846268108_R02C01', '908394505640_R07C01', '487666290134_R07C01', '141321329673_R05C01', '237003845025_R07C01', '283657642650_R03C01', '512936303868_R04C01', '154091750079_R04C01', '746064673880_R05C01', '318520677779_R04C01', '475314304290_R07C01', '280431937380_R07C01', '332567902971_R01C01', '263592413038_R05C01', '272226814535_R08C01', '367445200026_R05C01', '861950674936_R08C01', '281455998696_R08C01', '718358656399_R06C01', '471697814409_R02C01', '281993812131_R03C01', '409033458685_R06C01', '526297996719_R03C01', '897376287039_R07C01', '318520677779_R05C01', '225144595599_R07C01', '105073924092_R08C01', '847640913015_R05C01', '150054325625_R05C01', '549103146732_R03C01', '718358656399_R04C01', '418500757729_R08C01', '131837569795_R04C01', '620121161198_R02C01', '454253843253_R02C01', '533725106249_R03C01', '541728185176_R04C01', '793762720045_R05C01', '731721690709_R04C01', '930607649948_R04C01', '822619139198_R01C01', '177869651878_R02C01', '622559939130_R08C01', '753143667982_R04C01', '299259396834_R08C01', '410815277527_R08C01', '375697011604_R03C01', '338351716289_R06C01', '798469572936_R07C01', '955214508192_R08C01', '930607649948_R08C01', '841210177280_R05C01', '202332167680_R05C01', '836497085616_R08C01', '781812844328_R06C01', '703628837092_R05C01', '403005857801_R01C01', '705750859572_R05C01', '339942941850_R01C01', '402250799332_R01C01', '659213492573_R01C01', '425984903461_R06C01', '836211355859_R06C01', '162572071974_R07C01', '161574294830_R04C01', '231032635115_R06C01', '427463324693_R03C01', '441160472128_R02C01', '569962896597_R03C01', '293370276828_R02C01', '720887575043_R05C01', '736481597972_R01C01', '445008222423_R08C01', '357523778940_R06C01', '261240693481_R03C01', '224527397604_R05C01', '326197298338_R01C01', '697768424200_R07C01', '141321329673_R04C01', '410435493015_R06C01', '802747225807_R04C01', '205700129657_R07C01', '656285322665_R06C01', '941113024116_R05C01', '124087179681_R05C01', '717669440395_R07C01', '281455998696_R07C01', '832085762685_R04C01', '783558970293_R05C01', '822619139198_R04C01', '718358656399_R02C01', '542104796751_R04C01', '293370276828_R07C01', '111027746808_R01C01', '141607582061_R02C01', '232086152780_R08C01', '567888327914_R03C01', '941292468110_R08C01', '875777947523_R08C01', '574725596696_R03C01', '833388998512_R01C01', '633234357075_R07C01', '263592413038_R02C01', '256465182565_R08C01', '249340609164_R04C01', '179546779234_R02C01', '309483964587_R04C01', '614846268108_R07C01', '871397297930_R02C01', '283360003391_R07C01', '552800327810_R03C01', '857657432109_R08C01', '249340609164_R01C01', '370843053704_R07C01', '778220460366_R05C01', '724015031062_R02C01', '625347134215_R04C01', '259327475443_R06C01', '484531821918_R08C01', '908556847017_R05C01', '723926162887_R07C01', '312434972765_R08C01', '701354273035_R08C01', '873548359327_R05C01', '772039750222_R06C01', '264241292499_R03C01', '162314099453_R07C01', '940026495009_R04C01', '409033458685_R02C01', '842077214291_R01C01', '869639170419_R06C01', '539649633144_R05C01', '497640694542_R08C01', '395817284804_R02C01', '243280636096_R06C01', '299259396834_R07C01', '622655089219_R08C01', '771425908564_R03C01', '425984903461_R08C01', '339942941850_R03C01', '441480560435_R03C01', '846342189079_R02C01', '841210177280_R08C01', '290920959246_R02C01', '891759613014_R08C01', '596651647747_R02C01', '936726323263_R03C01', '519692205347_R06C01', '175080442509_R06C01', '618515929357_R04C01', '817452081759_R03C01', '165527333350_R03C01', '690887792452_R07C01', '638045872308_R01C01', '243280636096_R03C01', '347215041648_R07C01', '985681824931_R04C01', '884881661740_R06C01', '189815001486_R02C01', '357523778940_R02C01', '791757627635_R03C01', '565300924984_R07C01', '596025799733_R02C01', '318699302621_R02C01', '949340732366_R05C01', '435523318799_R01C01', '511043123237_R05C01', '798469572936_R08C01', '974942162848_R01C01', '809874509437_R02C01', '314970302917_R04C01', '593564749823_R04C01', '654256527889_R06C01', '555443417401_R06C01', '367445200026_R04C01', '828615934716_R02C01', '947995328253_R04C01', '809874509437_R07C01', '906629017116_R02C01', '812874808663_R04C01', '658675383171_R04C01', '896152806391_R07C01', '555693136394_R03C01', '497309779587_R05C01', '969644974702_R04C01', '469559925365_R05C01', '124052743399_R06C01', '720599104217_R08C01', '930607649948_R06C01', '691569131606_R02C01', '966667730534_R04C01', '395817284804_R04C01', '332567902971_R06C01', '802747225807_R08C01', '833981914157_R08C01', '742647105455_R08C01', '974942162848_R06C01', '659213492573_R02C01', '246344018115_R03C01', '617835642691_R05C01', '569962896597_R04C01', '556782804890_R04C01', '541728185176_R02C01', '526550316783_R04C01', '428101799801_R07C01', '551112914632_R08C01', '991694260988_R07C01', '235498722576_R04C01', '959285208680_R07C01', '654256527889_R07C01', '337407806513_R04C01', '264241292499_R06C01', '154306514811_R05C01', '705750859572_R04C01', '720599104217_R04C01', '787567883357_R04C01', '697730442812_R01C01', '537087445203_R02C01', '612873860160_R02C01', '423622823584_R01C01', '949465750547_R05C01', '593564749823_R05C01', '690887792452_R02C01', '326346266243_R05C01', '534942237018_R04C01', '259499946600_R03C01', '136636476349_R04C01', '555443417401_R03C01', '177869651878_R06C01', '729812414057_R05C01', '837336869336_R06C01', '125611104983_R02C01', '413109511303_R01C01', '556110265353_R03C01', '868489140011_R02C01', '161574294830_R07C01', '291039193735_R06C01', '617317247313_R02C01', '161192304356_R06C01', '102457222968_R01C01', '240317307947_R04C01', '658097089576_R02C01', '622559939130_R02C01', '703628837092_R07C01', '154306514811_R04C01', '717634550988_R02C01', '264241292499_R07C01', '726475683343_R05C01', '379699511633_R01C01', '807724809939_R05C01', '492603533119_R07C01', '105073924092_R06C01', '981599523195_R05C01', '342490166843_R02C01', '367919845967_R08C01', '659213492573_R06C01', '847993684057_R03C01', '555693136394_R08C01', '309483964587_R01C01', '488829691990_R08C01', '302829486490_R01C01', '134600764179_R05C01', '567248637098_R05C01', '828615934716_R01C01', '540340392819_R07C01', '893056782702_R05C01', '943809127801_R01C01', '299259396834_R04C01', '815580103324_R08C01', '534079262678_R02C01', '146322782850_R07C01', '216961848392_R03C01', '526550316783_R02C01', '134600764179_R01C01', '924450603759_R05C01', '216657920609_R08C01', '149763409002_R02C01', '175080442509_R02C01', '112109228129_R07C01', '256465182565_R02C01', '775863878388_R02C01', '677520708105_R01C01', '810733582457_R01C01', '959285208680_R08C01', '403648991923_R04C01', '489386889509_R02C01', '873910789671_R07C01', '347131797225_R01C01', '981050154523_R08C01', '914535196215_R01C01', '102192066051_R08C01', '997718225602_R01C01', '705619805512_R02C01', '193402818640_R03C01', '259327475443_R08C01', '622655089219_R01C01', '954171999232_R08C01', '970175547844_R03C01', '878735920474_R04C01', '992665766587_R01C01', '512936303868_R06C01', '936726323263_R01C01', '343620704356_R05C01', '872094993867_R02C01', '690887792452_R01C01', '156458444399_R01C01', '940939210159_R06C01', '284396789820_R08C01', '936726323263_R08C01', '921851048279_R06C01', '162572071974_R05C01', '751983786989_R03C01', '174625728020_R03C01', '537936333862_R01C01', '475273368356_R07C01', '550890872763_R05C01', '772507864220_R08C01', '425984903461_R05C01', '434040027563_R06C01', '884881661740_R07C01', '268974016901_R03C01', '717634550988_R04C01', '101675284524_R02C01', '448149926542_R08C01', '697730442812_R06C01', '545485318925_R06C01', '115869306133_R04C01', '390764654726_R01C01', '836211355859_R07C01', '717634550988_R05C01', '487262726316_R06C01', '101205079762_R02C01', '156458444399_R04C01', '174625728020_R06C01', '798469572936_R01C01', '106914218258_R05C01', '111027746808_R06C01', '303690110555_R02C01', '454253843253_R06C01', '873910789671_R02C01', '232086152780_R05C01', '985333917364_R08C01', '465696867211_R08C01', '731721690709_R07C01', '488829691990_R03C01', '189815001486_R05C01', '125470300855_R08C01', '433317916839_R07C01', '723926162887_R06C01', '947995328253_R05C01', '679966486485_R08C01', '278487170253_R07C01', '425984903461_R04C01', '424462208781_R02C01', '435523318799_R04C01', '689231219698_R04C01', '705619805512_R08C01', '750803737722_R03C01', '613678225918_R07C01', '980561656392_R08C01', '422239257348_R06C01', '280345865666_R05C01', '445944614274_R03C01', '540340392819_R08C01', '159642596676_R07C01', '721752585745_R06C01', '283360003391_R04C01', '748195694585_R08C01', '819577203021_R02C01', '655669055162_R05C01', '670710832239_R02C01', '101675284524_R03C01', '487666290134_R06C01', '249340609164_R03C01', '723503221258_R05C01', '511043123237_R07C01', '316824574906_R04C01', '542104796751_R02C01', '161538385077_R04C01', '549638808810_R01C01', '996217092342_R02C01', '370843053704_R02C01', '873521585701_R06C01', '238838281111_R07C01', '450347271120_R06C01', '750803737722_R07C01', '409033458685_R04C01', '924450603759_R02C01', '354487129470_R06C01', '136636476349_R01C01', '281455998696_R06C01', '686310040687_R03C01', '909965854582_R06C01', '353975803316_R06C01', '919577493927_R05C01', '515908295882_R04C01', '265365957013_R01C01', '530479725132_R02C01', '873521585701_R02C01', '503308981539_R07C01', '943809127801_R02C01', '780995603917_R02C01', '733445942814_R07C01', '249340609164_R08C01', '788100389565_R02C01', '996217092342_R08C01', '954171999232_R07C01', '156458444399_R07C01', '809128490680_R08C01', '615736654768_R04C01', '617317247313_R03C01', '935927726310_R04C01', '731009722943_R04C01', '161574294830_R05C01', '534079262678_R06C01', '272226814535_R03C01', '431787841683_R07C01', '871857211079_R03C01', '817452081759_R01C01', '498957085234_R06C01', '660513254160_R05C01', '537600205152_R08C01', '530479725132_R06C01', '943950722599_R04C01', '249340609164_R07C01', '720887575043_R02C01', '804927994325_R03C01', '884881661740_R03C01', '612873860160_R01C01', '459195675061_R02C01', '289429140398_R03C01', '353975803316_R04C01', '515908295882_R05C01', '852421958619_R06C01', '232086152780_R04C01', '746064673880_R08C01', '819577203021_R05C01', '809128490680_R01C01', '342490166843_R08C01', '442683914307_R02C01', '729812414057_R01C01', '780995603917_R03C01', '534079262678_R05C01', '194903682526_R03C01', '730528491305_R03C01', '112109228129_R03C01', '873521585701_R05C01', '419617709803_R04C01', '936440250716_R04C01', '567248637098_R01C01', '983183873249_R04C01', '565567925919_R04C01', '537936333862_R02C01', '767072274832_R07C01', '551112914632_R05C01', '335402317616_R06C01', '753143667982_R07C01', '772507864220_R02C01', '608388097842_R07C01', '610153679350_R04C01', '552800327810_R07C01', '111027746808_R03C01', '857657432109_R04C01', '617835642691_R04C01', '159915269274_R03C01', '301748988408_R04C01', '732310311550_R07C01', '455309526127_R05C01', '386267874836_R05C01', '418806447927_R07C01', '622655089219_R06C01', '530559649691_R06C01', '149763409002_R08C01', '852421958619_R02C01', '832085762685_R03C01', '617317247313_R01C01', '955214508192_R02C01', '194903682526_R01C01', '537936333862_R07C01', '104239741914_R04C01', '539719072775_R03C01', '780873329325_R02C01', '466858658091_R07C01', '935105934957_R07C01', '259327475443_R04C01', '105073924092_R01C01', '375697011604_R02C01', '985333917364_R02C01', '263592413038_R07C01', '846342189079_R01C01', '965801270332_R07C01', '748394829067_R02C01', '262668420369_R01C01', '225443631984_R04C01', '676415833218_R06C01', '259557760053_R08C01', '237003845025_R05C01', '617624186651_R08C01', '654256527889_R02C01', '584481139725_R06C01', '330872532165_R01C01', '954171999232_R05C01', '184693075170_R02C01', '722508017561_R02C01', '424462208781_R03C01', '753765410338_R02C01', '723503221258_R06C01', '574925169248_R01C01', '475273368356_R02C01', '375697011604_R07C01', '154347977474_R04C01', '955214508192_R03C01', '404183884905_R07C01', '942613364485_R03C01', '183529994180_R04C01', '861950674936_R04C01', '150054325625_R07C01', '746064673880_R02C01', '450347271120_R05C01', '871857211079_R08C01', '602831935409_R07C01', '868489140011_R06C01', '501674807024_R02C01', '395817284804_R07C01', '822619139198_R02C01', '443444850416_R01C01', '677520708105_R02C01', '617317247313_R07C01', '965801270332_R03C01', '948783561055_R05C01', '997796418290_R06C01', '701354273035_R07C01', '687358070409_R06C01', '184693075170_R07C01', '302829486490_R02C01', '748394829067_R04C01', '259557760053_R02C01', '731721690709_R01C01', '836211355859_R02C01', '952663155382_R04C01', '204656393806_R01C01', '448149926542_R05C01', '713504240303_R08C01', '395817284804_R03C01', '654256527889_R03C01', '421510609538_R08C01', '409033458685_R07C01', '569962896597_R05C01', '847993684057_R08C01', '483852006866_R03C01', '574725596696_R01C01', '721752585745_R02C01', '802747225807_R01C01', '765202665439_R07C01', '680938354872_R03C01', '584481139725_R03C01', '417559122788_R07C01', '853851804265_R06C01', '317698738521_R02C01', '249340609164_R02C01', '299259396834_R05C01', '225144595599_R01C01', '545485318925_R03C01', '793762720045_R06C01', '252540005934_R07C01', '555443417401_R05C01', '676598437273_R01C01', '500922670846_R05C01', '386267874836_R06C01', '751983786989_R02C01', '293370276828_R08C01', '549638808810_R08C01', '403648991923_R07C01', '610830863344_R07C01', '535548413560_R08C01', '297544515794_R06C01', '940026495009_R08C01', '437233300655_R05C01', '326346266243_R02C01', '555693136394_R04C01', '177869651878_R08C01', '202332167680_R02C01', '326346266243_R01C01', '284396789820_R02C01', '182309569467_R05C01', '644510089362_R07C01', '518221566828_R04C01', '845319605257_R05C01', '448149926542_R01C01', '310732956604_R07C01', '497974526103_R02C01', '356706463987_R03C01', '676598437273_R02C01', '335729275343_R01C01', '366937295416_R01C01', '610830863344_R03C01', '723503221258_R02C01', '656285322665_R05C01', '699944979302_R01C01', '617317247313_R06C01', '832085762685_R05C01', '383231842969_R05C01', '629956179614_R02C01', '709243521009_R01C01', '411281400237_R02C01', '293370276828_R01C01', '225328821563_R06C01', '622559939130_R06C01', '731009722943_R03C01', '949465750547_R06C01', '687358070409_R05C01', '273598663849_R04C01', '250095133527_R04C01', '509093861330_R01C01', '723926162887_R02C01', '333474679123_R03C01', '618515929357_R07C01', '483852006866_R06C01', '817436423280_R03C01', '936440250716_R03C01', '347131797225_R06C01', '541728185176_R01C01', '616827313631_R08C01', '280431937380_R02C01', '614846268108_R01C01', '442683914307_R04C01', '240378238690_R05C01', '897376287039_R08C01', '333474679123_R02C01', '707988188661_R07C01', '297447134307_R03C01', '782442065348_R04C01', '550890872763_R06C01', '105073924092_R02C01', '644510089362_R08C01', '240378238690_R08C01'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c253f",
   "metadata": {},
   "source": [
    "### <font color='darkblue'> Working with [Deep Graph Library](https://www.dgl.ai/)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93fc6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "g = dgl.from_networkx(G , node_attrs=['idx' , 'label'])\n",
    "g.ndata['feat'] = torch.Tensor(w1.iloc[g.ndata['idx'].numpy()].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c1b9df8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check that our graph is preserved\n",
    "\n",
    "node_0 = phenotypes.iloc[g.ndata['idx'].numpy()].iloc[0]\n",
    "\n",
    "node_0_edges = []\n",
    "for e1 , e2 in G.edges(node_0.name) :  \n",
    "    node_0_edges.append(e2)\n",
    "        \n",
    "# Note : DGL will reorder the nodes if integers present in the node name\n",
    "\n",
    "sorted(list(phenotypes.iloc[g.ndata['idx'].numpy()].iloc[g.out_edges(0)[1].numpy()].index)) == sorted(node_0_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cb887",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Graph Convolutional Network </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a799e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim,  hidden_feats, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.gcnlayers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.num_layers = len(hidden_feats) + 1\n",
    "        \n",
    "        for layers in range(self.num_layers) :\n",
    "            if layers < self.num_layers -1 :\n",
    "                if layers == 0 : \n",
    "                    self.gcnlayers.append(\n",
    "                        GraphConv(input_dim , hidden_feats[layers])\n",
    "                    )\n",
    "                else :\n",
    "                    self.gcnlayers.append(\n",
    "                        GraphConv(hidden_feats[layers-1] , hidden_feats[layers])\n",
    "                    )\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_feats[layers]))\n",
    "            else : \n",
    "                self.gcnlayers.append(\n",
    "                    GraphConv(hidden_feats[layers-1] , num_classes)\n",
    "                )\n",
    "                \n",
    "        self.drop = nn.Dropout(0.05)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including the input layer)\n",
    "        \n",
    "        for layers in range(self.num_layers) : \n",
    "            if layers == self.num_layers - 1 : \n",
    "                h = self.gcnlayers[layers](g , h)\n",
    "            else : \n",
    "                h = self.gcnlayers[layers](g, h)\n",
    "                h = self.drop(F.relu(h))\n",
    "            \n",
    "        score = self.drop(h)\n",
    "            \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba0721",
   "metadata": {},
   "source": [
    "### <font color='darkblue'> Training & Evaluation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e7ac6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve , average_precision_score , recall_score ,  PrecisionRecallDisplay\n",
    "\n",
    "def train(g, h, train_split , val_split , device ,  model , labels , epochs , lr , patience):\n",
    "    # loss function, optimizer and scheduler\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr , weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    consecutive_epochs_without_improvement = 0\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss   = []\n",
    "    \n",
    "    # training loop\n",
    "    train_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        logits  = model(g, h, device)\n",
    "\n",
    "        loss = loss_fcn(logits[train_split], labels[train_split].float())\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch % 5) == 0 :\n",
    "            \n",
    "            _, predicted = torch.max(logits[train_split], 1)\n",
    "            _, true = torch.max(labels[train_split] , 1)\n",
    "            train_acc = (predicted == true).float().mean().item()\n",
    "\n",
    "            valid_loss , valid_acc , valid_f1 , valid_PRC , valid_SNS , _ , _ = evaluate(val_split, device, g , h, model , labels)\n",
    "            print(\n",
    "                \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                    epoch, loss.item() , train_acc, valid_acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check for early stopping with waiting\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                consecutive_epochs_without_improvement = 0\n",
    "            else:\n",
    "                consecutive_epochs_without_improvement += 1\n",
    "\n",
    "            if consecutive_epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping! No improvement for {patience*5} consecutive epochs.\")\n",
    "                break\n",
    "\n",
    "            val_loss.append(valid_loss.item())\n",
    "\n",
    "    fig , ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(train_loss  , label = 'Train Loss')\n",
    "    ax.plot(range(5 , len(train_loss)+1 , 5) , val_loss  , label = 'Validation Loss')\n",
    "    ax.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def evaluate(idx, device, g , h, model , labels):\n",
    "    model.eval()\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        logits = model(g, h, device)\n",
    "\n",
    "        loss = loss_fcn(logits[idx], labels[idx].float())\n",
    "\n",
    "        acc += (logits[idx].argmax(1) == labels[idx].argmax(1)).float().mean().item()\n",
    "        \n",
    "        logits_out = logits[idx].cpu().detach().numpy()\n",
    "        binary_out = (logits_out == logits_out.max(1).reshape(-1,1))*1\n",
    "        \n",
    "        labels_out = labels[idx].cpu().detach().numpy()\n",
    "        \n",
    "        PRC =  average_precision_score(labels_out , binary_out , average=\"weighted\")\n",
    "        SNS = recall_score(labels_out , binary_out , average=\"weighted\")\n",
    "        F1 = 2*((PRC*SNS)/(PRC+SNS))\n",
    "        \n",
    "    \n",
    "    return loss , acc , F1 , PRC , SNS , logits_out , labels_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a871ce31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=23676, out=512, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=512, out=128, normalization=both, activation=None)\n",
      "    (2): GraphConv(in=128, out=64, normalization=both, activation=None)\n",
      "    (3): GraphConv(in=64, out=2, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.05, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1000, num_edges=114692,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int32), 'label': Scheme(shape=(), dtype=torch.int8), 'feat': Scheme(shape=(23676,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 0.6940 | Train Acc. 0.5083 | Validation Acc. 0.5000 \n",
      "Epoch 00005 | Loss 0.7091 | Train Acc. 0.4979 | Validation Acc. 0.5000 \n",
      "Epoch 00010 | Loss 0.7096 | Train Acc. 0.5167 | Validation Acc. 0.5000 \n",
      "Epoch 00015 | Loss 0.7036 | Train Acc. 0.4979 | Validation Acc. 0.5000 \n",
      "Epoch 00020 | Loss 0.6875 | Train Acc. 0.5312 | Validation Acc. 0.5000 \n",
      "Epoch 00025 | Loss 0.6946 | Train Acc. 0.4229 | Validation Acc. 0.5000 \n",
      "Epoch 00030 | Loss 0.6862 | Train Acc. 0.5917 | Validation Acc. 0.5000 \n",
      "Epoch 00035 | Loss 0.6901 | Train Acc. 0.5771 | Validation Acc. 0.5000 \n",
      "Epoch 00040 | Loss 0.6900 | Train Acc. 0.5729 | Validation Acc. 0.5000 \n",
      "Epoch 00045 | Loss 0.6930 | Train Acc. 0.4958 | Validation Acc. 0.6667 \n",
      "Epoch 00050 | Loss 0.6901 | Train Acc. 0.4958 | Validation Acc. 0.5000 \n",
      "Epoch 00055 | Loss 0.7014 | Train Acc. 0.4146 | Validation Acc. 0.6167 \n",
      "Epoch 00060 | Loss 0.6920 | Train Acc. 0.5250 | Validation Acc. 0.5667 \n",
      "Epoch 00065 | Loss 0.6924 | Train Acc. 0.5333 | Validation Acc. 0.6000 \n",
      "Epoch 00070 | Loss 0.6860 | Train Acc. 0.5604 | Validation Acc. 0.5000 \n",
      "Epoch 00075 | Loss 0.6886 | Train Acc. 0.5750 | Validation Acc. 0.5333 \n",
      "Epoch 00080 | Loss 0.6853 | Train Acc. 0.5104 | Validation Acc. 0.5000 \n",
      "Epoch 00085 | Loss 0.6847 | Train Acc. 0.5479 | Validation Acc. 0.5000 \n",
      "Epoch 00090 | Loss 0.6912 | Train Acc. 0.5271 | Validation Acc. 0.5000 \n",
      "Epoch 00095 | Loss 0.6911 | Train Acc. 0.5229 | Validation Acc. 0.6750 \n",
      "Epoch 00100 | Loss 0.6951 | Train Acc. 0.4646 | Validation Acc. 0.5000 \n",
      "Epoch 00105 | Loss 0.6987 | Train Acc. 0.4625 | Validation Acc. 0.6250 \n",
      "Epoch 00110 | Loss 0.6848 | Train Acc. 0.6063 | Validation Acc. 0.6750 \n",
      "Epoch 00115 | Loss 0.6956 | Train Acc. 0.4604 | Validation Acc. 0.6583 \n",
      "Epoch 00120 | Loss 0.6911 | Train Acc. 0.5250 | Validation Acc. 0.5000 \n",
      "Epoch 00125 | Loss 0.6862 | Train Acc. 0.6104 | Validation Acc. 0.5000 \n",
      "Epoch 00130 | Loss 0.6889 | Train Acc. 0.5708 | Validation Acc. 0.5417 \n",
      "Epoch 00135 | Loss 0.6886 | Train Acc. 0.5167 | Validation Acc. 0.5000 \n",
      "Epoch 00140 | Loss 0.6806 | Train Acc. 0.6292 | Validation Acc. 0.6917 \n",
      "Epoch 00145 | Loss 0.6902 | Train Acc. 0.5750 | Validation Acc. 0.6667 \n",
      "Epoch 00150 | Loss 0.6862 | Train Acc. 0.5896 | Validation Acc. 0.6833 \n",
      "Epoch 00155 | Loss 0.6829 | Train Acc. 0.5771 | Validation Acc. 0.5000 \n",
      "Epoch 00160 | Loss 0.6891 | Train Acc. 0.5750 | Validation Acc. 0.6167 \n",
      "Epoch 00165 | Loss 0.6926 | Train Acc. 0.5104 | Validation Acc. 0.6083 \n",
      "Epoch 00170 | Loss 0.6837 | Train Acc. 0.5333 | Validation Acc. 0.6917 \n",
      "Epoch 00175 | Loss 0.6858 | Train Acc. 0.5896 | Validation Acc. 0.6667 \n",
      "Epoch 00180 | Loss 0.6919 | Train Acc. 0.5479 | Validation Acc. 0.6083 \n",
      "Epoch 00185 | Loss 0.6885 | Train Acc. 0.5938 | Validation Acc. 0.6917 \n",
      "Epoch 00190 | Loss 0.6896 | Train Acc. 0.5938 | Validation Acc. 0.6917 \n",
      "Epoch 00195 | Loss 0.6853 | Train Acc. 0.5750 | Validation Acc. 0.6750 \n",
      "Epoch 00200 | Loss 0.6875 | Train Acc. 0.5833 | Validation Acc. 0.6333 \n",
      "Epoch 00205 | Loss 0.6881 | Train Acc. 0.5229 | Validation Acc. 0.5917 \n",
      "Epoch 00210 | Loss 0.6861 | Train Acc. 0.5521 | Validation Acc. 0.6667 \n",
      "Epoch 00215 | Loss 0.6844 | Train Acc. 0.5729 | Validation Acc. 0.7083 \n",
      "Epoch 00220 | Loss 0.6850 | Train Acc. 0.5604 | Validation Acc. 0.5000 \n",
      "Epoch 00225 | Loss 0.6858 | Train Acc. 0.6271 | Validation Acc. 0.6833 \n",
      "Epoch 00230 | Loss 0.6866 | Train Acc. 0.5792 | Validation Acc. 0.6833 \n",
      "Epoch 00235 | Loss 0.6889 | Train Acc. 0.5271 | Validation Acc. 0.5000 \n",
      "Epoch 00240 | Loss 0.6848 | Train Acc. 0.6146 | Validation Acc. 0.6667 \n",
      "Epoch 00245 | Loss 0.6880 | Train Acc. 0.5750 | Validation Acc. 0.6750 \n",
      "Epoch 00250 | Loss 0.6849 | Train Acc. 0.5708 | Validation Acc. 0.6833 \n",
      "Epoch 00255 | Loss 0.6819 | Train Acc. 0.6104 | Validation Acc. 0.6750 \n",
      "Epoch 00260 | Loss 0.6839 | Train Acc. 0.6146 | Validation Acc. 0.7000 \n",
      "Epoch 00265 | Loss 0.6880 | Train Acc. 0.5375 | Validation Acc. 0.6833 \n",
      "Epoch 00270 | Loss 0.6886 | Train Acc. 0.6021 | Validation Acc. 0.7000 \n",
      "Epoch 00275 | Loss 0.6875 | Train Acc. 0.5563 | Validation Acc. 0.6333 \n",
      "Epoch 00280 | Loss 0.6885 | Train Acc. 0.5521 | Validation Acc. 0.7083 \n",
      "Epoch 00285 | Loss 0.6861 | Train Acc. 0.5333 | Validation Acc. 0.6917 \n",
      "Epoch 00290 | Loss 0.6856 | Train Acc. 0.6417 | Validation Acc. 0.6583 \n",
      "Epoch 00295 | Loss 0.6881 | Train Acc. 0.5646 | Validation Acc. 0.6667 \n",
      "Epoch 00300 | Loss 0.6861 | Train Acc. 0.5917 | Validation Acc. 0.6750 \n",
      "Epoch 00305 | Loss 0.6805 | Train Acc. 0.6188 | Validation Acc. 0.6917 \n",
      "Epoch 00310 | Loss 0.6835 | Train Acc. 0.6354 | Validation Acc. 0.6583 \n",
      "Epoch 00315 | Loss 0.6880 | Train Acc. 0.5708 | Validation Acc. 0.6750 \n",
      "Epoch 00320 | Loss 0.6870 | Train Acc. 0.5583 | Validation Acc. 0.6083 \n",
      "Epoch 00325 | Loss 0.6927 | Train Acc. 0.5125 | Validation Acc. 0.6000 \n",
      "Epoch 00330 | Loss 0.6822 | Train Acc. 0.6396 | Validation Acc. 0.6667 \n",
      "Epoch 00335 | Loss 0.6856 | Train Acc. 0.5167 | Validation Acc. 0.7000 \n",
      "Epoch 00340 | Loss 0.6827 | Train Acc. 0.5917 | Validation Acc. 0.7083 \n",
      "Epoch 00345 | Loss 0.6871 | Train Acc. 0.5917 | Validation Acc. 0.6167 \n",
      "Epoch 00350 | Loss 0.6840 | Train Acc. 0.6083 | Validation Acc. 0.5917 \n",
      "Epoch 00355 | Loss 0.6861 | Train Acc. 0.6063 | Validation Acc. 0.6083 \n",
      "Epoch 00360 | Loss 0.6896 | Train Acc. 0.5750 | Validation Acc. 0.6083 \n",
      "Epoch 00365 | Loss 0.6876 | Train Acc. 0.5583 | Validation Acc. 0.6250 \n",
      "Epoch 00370 | Loss 0.6814 | Train Acc. 0.6271 | Validation Acc. 0.6667 \n",
      "Epoch 00375 | Loss 0.6929 | Train Acc. 0.5021 | Validation Acc. 0.6917 \n",
      "Epoch 00380 | Loss 0.6813 | Train Acc. 0.6625 | Validation Acc. 0.7000 \n",
      "Epoch 00385 | Loss 0.6811 | Train Acc. 0.6104 | Validation Acc. 0.6917 \n",
      "Epoch 00390 | Loss 0.6813 | Train Acc. 0.6208 | Validation Acc. 0.7083 \n",
      "Epoch 00395 | Loss 0.6881 | Train Acc. 0.5521 | Validation Acc. 0.6917 \n",
      "Epoch 00400 | Loss 0.6902 | Train Acc. 0.5729 | Validation Acc. 0.6583 \n",
      "Epoch 00405 | Loss 0.6839 | Train Acc. 0.6000 | Validation Acc. 0.6667 \n",
      "Epoch 00410 | Loss 0.6854 | Train Acc. 0.6271 | Validation Acc. 0.6667 \n",
      "Epoch 00415 | Loss 0.6833 | Train Acc. 0.6021 | Validation Acc. 0.6583 \n",
      "Epoch 00420 | Loss 0.6839 | Train Acc. 0.6229 | Validation Acc. 0.6750 \n",
      "Epoch 00425 | Loss 0.6904 | Train Acc. 0.5688 | Validation Acc. 0.7083 \n",
      "Epoch 00430 | Loss 0.6889 | Train Acc. 0.5438 | Validation Acc. 0.6917 \n",
      "Epoch 00435 | Loss 0.6834 | Train Acc. 0.6063 | Validation Acc. 0.6750 \n",
      "Epoch 00440 | Loss 0.6852 | Train Acc. 0.6083 | Validation Acc. 0.6583 \n",
      "Epoch 00445 | Loss 0.6799 | Train Acc. 0.5979 | Validation Acc. 0.6750 \n",
      "Epoch 00450 | Loss 0.6855 | Train Acc. 0.6146 | Validation Acc. 0.7000 \n",
      "Epoch 00455 | Loss 0.6874 | Train Acc. 0.5354 | Validation Acc. 0.6917 \n",
      "Epoch 00460 | Loss 0.6843 | Train Acc. 0.6271 | Validation Acc. 0.7000 \n",
      "Epoch 00465 | Loss 0.6836 | Train Acc. 0.6479 | Validation Acc. 0.7000 \n",
      "Epoch 00470 | Loss 0.6847 | Train Acc. 0.5979 | Validation Acc. 0.6750 \n",
      "Epoch 00475 | Loss 0.6939 | Train Acc. 0.5188 | Validation Acc. 0.6750 \n",
      "Epoch 00480 | Loss 0.6843 | Train Acc. 0.6354 | Validation Acc. 0.6833 \n",
      "Epoch 00485 | Loss 0.6866 | Train Acc. 0.6042 | Validation Acc. 0.6917 \n",
      "Epoch 00490 | Loss 0.6891 | Train Acc. 0.5646 | Validation Acc. 0.7083 \n",
      "Epoch 00495 | Loss 0.6853 | Train Acc. 0.6000 | Validation Acc. 0.7083 \n",
      "Epoch 00500 | Loss 0.6807 | Train Acc. 0.6396 | Validation Acc. 0.7083 \n",
      "Epoch 00505 | Loss 0.6871 | Train Acc. 0.5979 | Validation Acc. 0.7083 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00510 | Loss 0.6848 | Train Acc. 0.5938 | Validation Acc. 0.7083 \n",
      "Epoch 00515 | Loss 0.6885 | Train Acc. 0.6000 | Validation Acc. 0.7083 \n",
      "Epoch 00520 | Loss 0.6886 | Train Acc. 0.5729 | Validation Acc. 0.7083 \n",
      "Epoch 00525 | Loss 0.6855 | Train Acc. 0.6083 | Validation Acc. 0.7083 \n",
      "Epoch 00530 | Loss 0.6865 | Train Acc. 0.6063 | Validation Acc. 0.7083 \n",
      "Epoch 00535 | Loss 0.6837 | Train Acc. 0.6125 | Validation Acc. 0.7083 \n",
      "Epoch 00540 | Loss 0.6872 | Train Acc. 0.6042 | Validation Acc. 0.7083 \n",
      "Epoch 00545 | Loss 0.6834 | Train Acc. 0.6063 | Validation Acc. 0.7000 \n",
      "Epoch 00550 | Loss 0.6875 | Train Acc. 0.6042 | Validation Acc. 0.7000 \n",
      "Epoch 00555 | Loss 0.6830 | Train Acc. 0.6146 | Validation Acc. 0.7000 \n",
      "Epoch 00560 | Loss 0.6860 | Train Acc. 0.5479 | Validation Acc. 0.7000 \n",
      "Epoch 00565 | Loss 0.6867 | Train Acc. 0.6375 | Validation Acc. 0.6917 \n",
      "Epoch 00570 | Loss 0.6832 | Train Acc. 0.5708 | Validation Acc. 0.6917 \n",
      "Epoch 00575 | Loss 0.6793 | Train Acc. 0.6188 | Validation Acc. 0.6917 \n",
      "Epoch 00580 | Loss 0.6908 | Train Acc. 0.5396 | Validation Acc. 0.6917 \n",
      "Epoch 00585 | Loss 0.6851 | Train Acc. 0.6271 | Validation Acc. 0.6917 \n",
      "Epoch 00590 | Loss 0.6827 | Train Acc. 0.6563 | Validation Acc. 0.6917 \n",
      "Epoch 00595 | Loss 0.6855 | Train Acc. 0.5563 | Validation Acc. 0.7000 \n",
      "Epoch 00600 | Loss 0.6891 | Train Acc. 0.5896 | Validation Acc. 0.7000 \n",
      "Epoch 00605 | Loss 0.6868 | Train Acc. 0.6188 | Validation Acc. 0.7000 \n",
      "Epoch 00610 | Loss 0.6915 | Train Acc. 0.5271 | Validation Acc. 0.7000 \n",
      "Epoch 00615 | Loss 0.6777 | Train Acc. 0.6458 | Validation Acc. 0.7000 \n",
      "Epoch 00620 | Loss 0.6890 | Train Acc. 0.5458 | Validation Acc. 0.7083 \n",
      "Epoch 00625 | Loss 0.6840 | Train Acc. 0.6000 | Validation Acc. 0.7083 \n",
      "Epoch 00630 | Loss 0.6846 | Train Acc. 0.6104 | Validation Acc. 0.7083 \n",
      "Epoch 00635 | Loss 0.6856 | Train Acc. 0.5938 | Validation Acc. 0.7083 \n",
      "Epoch 00640 | Loss 0.6906 | Train Acc. 0.5208 | Validation Acc. 0.7083 \n",
      "Epoch 00645 | Loss 0.6898 | Train Acc. 0.5208 | Validation Acc. 0.7083 \n",
      "Epoch 00650 | Loss 0.6841 | Train Acc. 0.6604 | Validation Acc. 0.7083 \n",
      "Epoch 00655 | Loss 0.6889 | Train Acc. 0.5625 | Validation Acc. 0.7083 \n",
      "Epoch 00660 | Loss 0.6803 | Train Acc. 0.6354 | Validation Acc. 0.7083 \n",
      "Epoch 00665 | Loss 0.6832 | Train Acc. 0.6479 | Validation Acc. 0.7083 \n",
      "Epoch 00670 | Loss 0.6860 | Train Acc. 0.6021 | Validation Acc. 0.7083 \n",
      "Epoch 00675 | Loss 0.6804 | Train Acc. 0.6438 | Validation Acc. 0.7083 \n",
      "Epoch 00680 | Loss 0.6861 | Train Acc. 0.6313 | Validation Acc. 0.7083 \n",
      "Epoch 00685 | Loss 0.6877 | Train Acc. 0.5771 | Validation Acc. 0.7083 \n",
      "Epoch 00690 | Loss 0.6868 | Train Acc. 0.5750 | Validation Acc. 0.7083 \n",
      "Epoch 00695 | Loss 0.6817 | Train Acc. 0.6625 | Validation Acc. 0.7083 \n",
      "Epoch 00700 | Loss 0.6825 | Train Acc. 0.6417 | Validation Acc. 0.7083 \n",
      "Epoch 00705 | Loss 0.6912 | Train Acc. 0.5479 | Validation Acc. 0.7083 \n",
      "Epoch 00710 | Loss 0.6856 | Train Acc. 0.6063 | Validation Acc. 0.7083 \n",
      "Epoch 00715 | Loss 0.6826 | Train Acc. 0.6208 | Validation Acc. 0.7083 \n",
      "Epoch 00720 | Loss 0.6929 | Train Acc. 0.4813 | Validation Acc. 0.7083 \n",
      "Epoch 00725 | Loss 0.6871 | Train Acc. 0.6333 | Validation Acc. 0.7083 \n",
      "Early stopping! No improvement for 250 consecutive epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs30lEQVR4nO3deXxV9bnv8c+zM5KBQEgYgzKjyEyUghNoax1QjlStlKpoq1dvj3a4p9r2WvXYejoc7621raW2Dq1ypWgt1TpWlGLrxFBGAUHGMCQhIROZs3/3j72y2TsDCWFDssL3/XrlxZr2Ws9eOzz57ee31m+Zcw4REfG/QGcHICIisaGELiLSTSihi4h0E0roIiLdhBK6iEg3oYQuItJNKKFLFDN7zcxuivW2ncnMdprZZ0/AfpeZ2Ve96Xlm9mZ7tu3AcU4zswozi+torHJqUELvBrz/7I0/QTOripifdyz7cs5d5pz7fay37YrM7DtmtryF5VlmVmtmY9u7L+fcQufcJTGKK+oPkHNut3MuzTnXEIv9NzmWM7MRsd6vdA4l9G7A+8+e5pxLA3YDV0YsW9i4nZnFd16UXdKzwHQzG9pk+fXAeufchk6ISaTDlNC7MTObYWZ5ZnaPmR0AnjKz3mb2VzMrNLND3nROxGsiywjzzewfZvawt+0OM7usg9sONbPlZlZuZm+Z2a/M7NlW4m5PjD8ws396+3vTzLIi1t9gZrvMrMjM/ndr58c5lwe8DdzQZNWNwB/aiqNJzPPN7B8R858zs81mVmpmvwQsYt1wM3vbi++gmS00s17eumeA04CXvW9Yd5vZEK8lHe9tM9DMXjKzYjPbZma3Ruz7ATNbbGZ/8M7NRjPLbe0ctMbMMrx9FHrn8l4zC3jrRpjZ3733dtDM/ugtNzP7mZkVmFmZma0/lm85cvyU0Lu//kAmcDpwG6HP/Clv/jSgCvjlUV4/FdgCZAE/BZ4wM+vAtv8P+AjoAzxA8yQaqT0xfgm4GegLJAL/AWBmY4Bfe/sf6B2vxSTs+X1kLGY2GpjoxXus56pxH1nAi8C9hM7Fp8C5kZsAP/LiOxMYTOic4Jy7gehvWT9t4RCLgDzv9dcA/2VmF0Wsv8rbphfwUntibsEvgAxgGHAhoT9yN3vrfgC8CfQmdG5/4S2/BLgAGOW99jqgqAPHlo5yzumnG/0AO4HPetMzgFog+SjbTwQORcwvA77qTc8HtkWsSwEc0P9YtiWUDOuBlIj1zwLPtvM9tRTjvRHz/xN43Zu+D1gUsS7VOwefbWXfKUAZMN2bfwj4SwfP1T+86RuBDyK2M0IJ+Kut7PffgH+19Bl680O8cxlPKPk3AOkR638EPO1NPwC8FbFuDFB1lHPrgBFNlsV552xMxLL/ASzzpv8APA7kNHndRcAnwGeAQGf/XzgVf9RC7/4KnXPVjTNmlmJmv/G+RpcBy4Fe1voVFAcaJ5xzld5k2jFuOxAojlgGsKe1gNsZ44GI6cqImAZG7ts5d5ijtBK9mJ4HbvS+TcwjlLA6cq4aNY3BRc6bWT8zW2Rme739PkuoJd8ejeeyPGLZLmBQxHzTc5Nsx9Z/kgUkePtt6Rh3E/oj9ZFX0rkFwDn3NqFvA78CCszscTPreQzHleOkhN79NR1O838Bo4GpzrmehL4iQ0SN9wTYD2SaWUrEssFH2f54YtwfuW/vmH3aeM3vCZUHPgekAy8fZxxNYzCi3+9/Efpcxnn7/XKTfR5tCNR9hM5lesSy04C9bcR0LA4CdYRKTc2O4Zw74Jy71Tk3kFDL/THzrpRxzj3qnJtC6JvBKODbMYxL2qCEfupJJ1QLLjGzTOD+E31A59wuYCXwgJklmtk04MoTFOMLwCwzO8/MEoEHafv3/F2ghFAZYZFzrvY443gFOMvM5ngt47sIlZ4apQMVQKmZDaJ50ssnVLtuxjm3B3gP+JGZJZvZeOArhFr5HZXo7SvZzJK9ZYuBh8ws3cxOB77VeAwzuzaic/gQoT9AQTM728ymmlkCcBioBoLHEZccIyX0U88jQA9CrbAPgNdP0nHnAdMIlT9+CPwRqGll20foYIzOuY3A1wh1au4nlHDy2niNI1RmOd3797jicM4dBK4Ffkzo/Y4E/hmxyX8Ck4FSQsn/xSa7+BFwr5mVmNl/tHCIuYTq6vuAPwP3O+feak9srdhI6A9X48/NwJ2EkvJ24B+EzueT3vZnAx+aWQWhTtevO+e2Az2B3xI657sIvff/Po645BiZ15khclJ5l7ptds6d8G8IIqcKtdDlpPC+jg83s4CZXQrMBpZ0clgi3YruHJSTpT+h0kIfQiWQO5xz/+rckES6F5VcRES6CZVcRES6iU4ruWRlZbkhQ4Z01uFFRHxp1apVB51z2S2t67SEPmTIEFauXNlZhxcR8SUz29XaOpVcRES6CSV0EZFuQgldRKSbaLOGbmZPArOAAudcs8HqzSyD0BgPp3n7e9g591SsAxWRjqmrqyMvL4/q6uq2N5YuIzk5mZycHBISEtr9mvZ0ij5NaEjMP7Sy/mvAx865K80sG9hiZgsjBjgSkU6Ul5dHeno6Q4YMofVnk0hX4pyjqKiIvLw8hg5t+oTE1rVZcnHOLQeKj7YJkO4NEZrmbVvf7ghE5ISqrq6mT58+SuY+Ymb06dPnmL9VxeKyxV8SGnFtH6FhQb/onNOQmSJdiJK5/3TkM4tFp+jngTWEnqQyEfhla08pMbPbzGylma0sLCzs8AHf3VrIrqLDHX69iEh3FIuEfjPwogvZBuwAzmhpQ+fc4865XOdcbnZ2izc6tcsNT3zEhf+9rMOvF5GTp6ioiIkTJzJx4kT69+/PoEGDwvO1tUfvalu5ciV33XXXMR1vyJAhHDx48HhC9q1YlFx2AxcD75pZP0KP7Noeg/2KSDfQp08f1qxZA8ADDzxAWloa//EfR57bUV9fT3x8y6koNzeX3NzckxFmt9BmC93MngPeB0abWZ6ZfcXMbjez271NfgBMN7P1wFLgHu+JLSIiLZo/fz633347U6dO5e677+ajjz5i2rRpTJo0ienTp7NlyxYAli1bxqxZs4DQH4NbbrmFGTNmMGzYMB599NF2H2/nzp1cdNFFjB8/nosvvpjdu3cD8PzzzzN27FgmTJjABReEHhm7ceNGzjnnHCZOnMj48ePZunVrjN/9idNmC905N7eN9fuAS2IWUTtNsq3w0+Fw3R9gyLkn+/AivvSfL2/k431lMd3nmIE9uf/Ks475dXl5ebz33nvExcVRVlbGu+++S3x8PG+99Rbf+973+NOf/tTsNZs3b+add96hvLyc0aNHc8cdd7TrOu0777yTm266iZtuuoknn3ySu+66iyVLlvDggw/yxhtvMGjQIEpKSgBYsGABX//615k3bx61tbU0NDQc83vrLL59wEWAIFQehHrdLCHiR9deey1xcXEAlJaWctNNN7F161bMjLq6uhZfc8UVV5CUlERSUhJ9+/YlPz+fnJycFreN9P777/Pii6FHt95www3cfffdAJx77rnMnz+f6667jjlz5gAwbdo0HnroIfLy8pgzZw4jR46Mxds9KXyb0ION1SJdISnSbh1pSZ8oqamp4envf//7zJw5kz//+c/s3LmTGTNmtPiapKSk8HRcXBz19cd3y8uCBQv48MMPeeWVV5gyZQqrVq3iS1/6ElOnTuWVV17h8ssv5ze/+Q0XXXTRcR3nZPHtWC4NjaEH/fN1SERaVlpayqBBgwB4+umnY77/6dOns2jRIgAWLlzI+eefD8Cnn37K1KlTefDBB8nOzmbPnj1s376dYcOGcddddzF79mzWrVsX83hOFN8m9CDeRfdqoYv43t133813v/tdJk2adNytboDx48eTk5NDTk4O3/rWt/jFL37BU089xfjx43nmmWf4+c9/DsC3v/1txo0bx9ixY5k+fToTJkxg8eLFjB07lokTJ7JhwwZuvPHG447nZOm0Z4rm5ua6jj7gYsh3XmGM7eTVpO/BF5+FM6+McXQi3cemTZs488wzOzsM6YCWPjszW+Wca/FaTt+20FVyERGJ5v+E7pTQRUTAxwk9fJVLUDV0ERHwcUJv0GWLIiJRfJvQj1zlopKLiAj4OaE7dYqKiETybUJXp6iIP8ycOZM33ngjatkjjzzCHXfc0eprZsyYQeNlzZdffnl4nJVIDzzwAA8//PBRj71kyRI+/vjj8Px9993HW2+9dQzRtyxy0LCuxP8JXS10kS5t7ty54bs0Gy1atIi5c4867l/Yq6++Sq9evTp07KYJ/cEHH+Szn/1sh/blB75N6BrLRcQfrrnmGl555ZXwwyx27tzJvn37OP/887njjjvIzc3lrLPO4v7772/x9ZEPrHjooYcYNWoU5513XniIXYDf/va3nH322UyYMIEvfOELVFZW8t577/HSSy/x7W9/m4kTJ/Lpp58yf/58XnjhBQCWLl3KpEmTGDduHLfccgs1NTXh491///1MnjyZcePGsXnz5na/1+eeey585+k999wDQENDA/Pnz2fs2LGMGzeOn/3sZwA8+uijjBkzhvHjx3P99dcf41ltmY8H59Kt/yLH7LXvwIH1sd1n/3Fw2Y9bXZ2Zmck555zDa6+9xuzZs1m0aBHXXXcdZsZDDz1EZmYmDQ0NXHzxxaxbt47x48e3uJ9Vq1axaNEi1qxZQ319PZMnT2bKlCkAzJkzh1tvvRWAe++9lyeeeII777yTq666ilmzZnHNNddE7au6upr58+ezdOlSRo0axY033sivf/1rvvGNbwCQlZXF6tWreeyxx3j44Yf53e9+1+Zp2LdvH/fccw+rVq2id+/eXHLJJSxZsoTBgwezd+9eNmzYABAuH/34xz9mx44dJCUltVhS6gjfttBVchHxj8iyS2S5ZfHixUyePJlJkyaxcePGqPJIU++++y5XX301KSkp9OzZk6uuuiq8bsOGDZx//vmMGzeOhQsXsnHjxqPGs2XLFoYOHcqoUaMAuOmmm1i+fHl4feNQulOmTGHnzp3teo8rVqxgxowZZGdnEx8fz7x581i+fDnDhg1j+/bt3Hnnnbz++uv07Bl65PL48eOZN28ezz77bKtPbDpWPm6hq1NU5JgdpSV9Is2ePZtvfvObrF69msrKSqZMmcKOHTt4+OGHWbFiBb1792b+/PlUV3fs+Qbz589nyZIlTJgwgaeffpply5YdV7yNw/TGYoje3r17s3btWt544w0WLFjA4sWLefLJJ3nllVdYvnw5L7/8Mg899BDr168/7sSuFrqInHBpaWnMnDmTW265Jdw6LysrIzU1lYyMDPLz83nttdeOuo8LLriAJUuWUFVVRXl5OS+//HJ4XXl5OQMGDKCuro6FCxeGl6enp1NeXt5sX6NHj2bnzp1s27YNgGeeeYYLL7zwuN7jOeecw9///ncOHjxIQ0MDzz33HBdeeCEHDx4kGAzyhS98gR/+8IesXr2aYDDInj17mDlzJj/5yU8oLS2loqLiuI4PaqGLyEkyd+5crr766nDpZcKECUyaNIkzzjiDwYMHc+65R3+U5OTJk/niF7/IhAkT6Nu3L2effXZ43Q9+8AOmTp1KdnY2U6dODSfx66+/nltvvZVHH3003BkKkJyczFNPPcW1115LfX09Z599NrfffnuzYx7N0qVLo56W9Pzzz/PjH/+YmTNn4pzjiiuuYPbs2axdu5abb76ZoDdMyY9+9CMaGhr48pe/TGlpKc457rrrrg5fyRPJt8PnxlPPtuQb4aJ74YJvxzg6ke5Dw+f61ykzfK4G5xIRiebjhK6xXEREIvk2oYOBBdQpKtIOnVValY7ryGfm44QOWJxa6CJtSE5OpqioSEndR5xzFBUVkZycfEyv8+1VLgAE4nSnqEgbcnJyyMvLo7CwsLNDkWOQnJwcdRVNe/g7oavkItKmhIQEhg4d2tlhyEnQDUouaqGLiEA7ErqZPWlmBWa24SjbzDCzNWa20cz+HtsQjyKgFrqISKP2tNCfBi5tbaWZ9QIeA65yzp0FXBuTyNpDnaIiImFtJnTn3HKg+CibfAl40Tm329u+IEaxtU2doiIiYbGooY8CepvZMjNbZWY3trahmd1mZivNbGVMetwtTiUXERFPLBJ6PDAFuAL4PPB9MxvV0obOucedc7nOudzs7OzjP7IFVHIREfHE4rLFPKDIOXcYOGxmy4EJwCcx2PfRBeI0louIiCcWLfS/AOeZWbyZpQBTgU0x2G/b1EIXEQlrs4VuZs8BM4AsM8sD7gcSAJxzC5xzm8zsdWAdEAR+55xr9RLHmAqohi4i0qjNhO6cm9uObf4b+O+YRHQsdGORiEiYz+8UVclFRKSR7xJ61IhxKrmIiIT5LqFHUclFRCTMdwk9akhnjeUiIhLmu4QeRS10EZEw3yX0qGeuqFNURCTMdwk9ijpFRUTCfJfQo65yUclFRCTMdwk9ilroIiJhvkvozWvoaqGLiIAfE3rUZYt6YpGISCPfJfQopuvQRUQa+S6hO5p2iiqhi4iADxN6FD3gQkQkzHcJPaqGrhuLRETCfJfQowR0HbqISCN/J3R1ioqIhPk8oatTVESkke8SerPr0NVCFxEB/JjQm122qBq6iAj4MKFHCejWfxGRRr5L6NGXLarkIiLSyHcJPYquQxcRCfNdQo8abVGdoiIiYb5L6FF02aKISJjvEnrUE4sCcU2K6iIip642E7qZPWlmBWa2oY3tzjazejO7JnbhtUF3ioqIhLWnhf40cOnRNjCzOOAnwJsxiOmomj+xSAldRATakdCdc8uB4jY2uxP4E1AQi6COHk/EjDpFRUTCjruGbmaDgKuBXx9/OMd6cHWKiog0ikWn6CPAPc61fcummd1mZivNbGVhYWHHjtbsmaJBdYyKiADxMdhHLrDIzACygMvNrN45t6Tphs65x4HHAXJzc48/C1tc444hdHwRkVPWcSd059zQxmkzexr4a0vJPFaiBucKeF8wXAM+vAJTRCSm2kzoZvYcMAPIMrM84H4gAcA5t+CERtcW85J4sAHiEjo1FBGRztZmQnfOzW3vzpxz848rmnYdI2ImXHJRx6iIiL/rFAEvoevSRRER/yX06BuLGlvoGhNdRMR3CT1KQAldRKSR7xJ61OBckZ2iIiKnOP8l9MgZi7xsUUTk1Oa7hB5FnaIiImG+S+gtX7aoGrqIiO8SepSArkMXEWnku4Qedeu/OkVFRMJ8l9CjqOQiIhLmv4QeNXyuWugiIo38l9AjaSwXEZEw3yX0qOvQdaeoiEiY/xJ6S5ctquQiIuK/hB5Fd4qKiIT5LqFHP7GosYWukouIiO8SehS10EVEwnyX0KNq6OoUFREJ811Cj6JOURGRMN8ldA2fKyLSMt8l9CgaPldEJMx3CT36iUWqoYuINPJhQo+YaRzLRQldRMR/CT2KOkVFRML8ndD1gAsRkTB/J3Q94EJEJMx3Cb3lZ4oqoYuItJnQzexJMyswsw2trJ9nZuvMbL2ZvWdmE2IfZis0louISFh7WuhPA5ceZf0O4ELn3DjgB8DjMYirVS0+U1RXuYiIEN/WBs655WY25Cjr34uY/QDIiUFc7aNOURGRsFjX0L8CvNbaSjO7zcxWmtnKwsLCDh0guoauTlERkUYxS+hmNpNQQr+ntW2cc48753Kdc7nZ2dkdOk70WC5qoYuINGqz5NIeZjYe+B1wmXOuKBb7bBeN5SIiEnbcLXQzOw14EbjBOffJ8Yd0dBrLRUSkZW220M3sOWAGkGVmecD9QAKAc24BcB/QB3jMzADqnXO5JyrgKHrAhYhIWHuucpnbxvqvAl+NWURtaHE8dJVcRET8d6doFD3gQkQkzHcJvcVniqqFLiLiv4QeRZ2iIiJhPkzoEU103SkqIhLmw4QewTQ4l4hII98l9Ohb/81bqBa6iIj/EnrkjFnoShd1ioqI+C+hN2NxaqGLiODDhB5VcoFQx6iuchER8V9Cb8biVHIREcGHCd3RpIluAbXQRUTwYUJvJqBOURER8GFCb1ZDV6eoiAjgw4TejDpFRUQAHyb0FlvoKrmIiPgwoTftFA2o5CIiAj5M6M1YQGO5iIjgw4TevOQSUAtdRAQfJvRm1CkqIgJ0h4SuTlEREaA7JHR1ioqIAD5M6C3W0NVCFxHxX0JvxlRDFxEBHyb05tehq4UuIgJ+TOgtjuWiFrqIiO8SejPqFBURAdqR0M3sSTMrMLMNraw3M3vUzLaZ2Tozmxz7MI9o2kDXZYsiIiHtaaE/DVx6lPWXASO9n9uAXx9/WMdAD7gQEQHakdCdc8uB4qNsMhv4gwv5AOhlZgNiFWAL8UQvCKiFLiICsamhDwL2RMznectODrXQRUSAk9wpama3mdlKM1tZWFjYoX00q6GrU1REBIhNQt8LDI6Yz/GWNeOce9w5l+ucy83Ozo7BoVGnqIiIJxYJ/SXgRu9ql88Apc65/THYb4s0fK6ISMvi29rAzJ4DZgBZZpYH3A8kADjnFgCvApcD24BK4OYTFWxIS52iqqGLiLSZ0J1zc9tY74CvxSyiY6VOURERwId3ijYruahTVEQE8GFCb0adoiIigA8Tui5bFBFpme8SejN6wIWICODDhK7hc0VEWua7hN5MQFe5iIiADxN6s8G51CkqIgL4MKE3o05RERHAhwm9+QMu1CkqIgJ+TOgtdooqoYuI+C6hNxOIayHLi4icenyX0F3TootKLiIigA8TejPqFBURAfyY0FuqoauFLiLiw4TelB5wISIC+DChtzw4l+4UFRHxXUJvxuJC/+qpRSJyivNdQm/+gAvvLajsIiKnOP8l9Iiii3MuooWuhC4ipzbfJfRmTC10ERHwYUKPLLk0BF2oUxTUQheRU57vEnqkhsiSi650EZFTnO8SemSfaH1DRAtdCV1ETnG+S+iR6oPuSA1dJRcROcX5LqFHPrGoITKhq1NURE5xvkvokeqDQXWKioh4fJfQm9XQ1SkqIgK0M6Gb2aVmtsXMtpnZd1pYf5qZvWNm/zKzdWZ2eexD9bR22aJKLiJyimszoZtZHPAr4DJgDDDXzMY02exeYLFzbhJwPfBYrANtSahTVCUXERFoXwv9HGCbc267c64WWATMbrKNA3p60xnAvtiF2PRAkZ2iQV22KCLiaU9CHwTsiZjP85ZFegD4spnlAa8Cd7a0IzO7zcxWmtnKwsLCDoQbra7BgVloRi10ETnFxapTdC7wtHMuB7gceMbMmu3bOfe4cy7XOZebnZ3doQM1u/VfnaIiIkD7EvpeYHDEfI63LNJXgMUAzrn3gWQgKxYBHk29OkVFRMLak9BXACPNbKiZJRLq9HypyTa7gYsBzOxMQgn9+GsqLYhuoQfVKSoi4mkzoTvn6oF/B94ANhG6mmWjmT1oZld5m/0v4FYzWws8B8x3rtmjKGIueiwXJXQRObXFt2cj59yrhDo7I5fdFzH9MXBubENrJZaI6eixXFRDF5FTm//uFI1o+NdrLBcRkTDfJfRIug5dROQI3yX0VsdyUaeoiJzifJfQI+myRRGRI3yX0COvndFYLiIiR/guoUcKXYfe2CmqGrqInNp8mNCPNNHf21YEASV0ERHwZUI/4vlVeSq5iIh4fJfQm95/WhP0RltUp6iInOJ8l9CbuvJX7wNQUVXTyZGIiHQu3yX0xgb6g7PPAiBIqIX+xLvb+OOK3ewprmTnwcM0BB1/WbM3NMTuMSitqotluCIiJ43/ErqXn6cO7QNAj8REAHbnH+JXL77FnJ++yMUPL+Vnf/uEry9aw/Dvvcpr6/fz4uo8gl5yX737EEUVNSzdlM+GvaXhfa/dU8KE/3yTP67YDUBN/fGVcZxzlFXXcd9fNlBaeeQPRWVtPSdh7DIROcW0a3CursgMPvjuxSSX74Lfwf9JXBBeF3RG8XvpXJGYQaHrReHiDMpcKr97MZ5eaT3YXxFkqYunjjjqiGdt/96sO1DFkL4ZXBmo5p0/f8T2f2aw9WAtU0f0Y3B2L8bkZPL6pmLqiadv73SWbSvhykmnMXloNq9uKmLXoTomD80mq2cqJVX1LN1UwJsbD1BeUw+EHsbxtZkjuO8vG3lrUz5n9E9n3tTTGJ6dRn3QccGobJxzlNfUk19azfynVvDYvMnsOHiYmvoGLhnTn96piRysqKGqtoHBmSm8u7WQlMR4CsqqWfjhbrYXVvDo3Elcs+B9fvmlScwaP5BVuw6xv7SK9XmlzDyjL2cO6ElCnJGSGProN+4rJTEuwMh+6fx2+XbW7S3l/1w7gcT40N/60qo69pdW8eq6/Vw9OYehWankHapkUK8e7C6uZEBGDxLjAzjnOFRZx+PLt7N61yEW3z4t/Hm8/2kRAzKSyeiRQO/UxGaf5fq8Uob3TSUlMZ7iw7X0TknAGp9EFeFwTT3r8koZkJHMkKxUyqvrMDPSko78Gm/YW8ofV+xh9sSB5A7JbNfvUlFFDQ1BR21DkMzURDbsLaNfzyRO75NK8eFaCsqrWbGjmNH9e3LO0Ez2llSRlZbInuIqBmQks73wMKVVdUwb3ocPdxRxRv+eZEa8z/yyamrqgiQlBKitDzI4M4XqugZ2F1dyep8UkuLj2LC3lOz0JAD69UwGQg2CvENVLPxwN/9+0Yio99mSQ4druXfJBmaNH8Bl4wZErSutrAODuIDx5D92UFBezQ//bVx4/do9JYzql06PxNBFBjsOHqZncjx90pJYs6eEtzfl861LRrOnuJKN+8r4/Fn9MDOCQcehylr6pCXxwfYi+qQmMrJfervOO8D+0iq+v2QjN00/nfNHtv3Qm4qaelbsKObCUdkEAsauosO8s7mAy8cPoPhwLSt2FHPDtCFRrymvruOD7cVcdEZfgs6REHekHbtxXymDM1PomZwAhM75rqJKhmSlAlDXEGTxyj3MGjeQ1KQ44uMClFXXsXFvGdOG92kWX31DkLV5pUw5vTfVdQ3EB4xthRU0BB2vbzjAHO//0IlindVSzM3NdStXrjzm163efYin/7mT715+BgMyeoRGWVz2I3YfqubR1TUkUUe2lZBNKdlWwrAeh0muOUg6lcTTQAL1JNqJ7UCtc3HUE0cdoX/rCf3xqA8vjydIgAaMBgIECTC0b0+KKxvIr6ijwQWII0iKhfoFCl0GJS6N7F7p7C6pxQXimXx6Jut37CfZaql0SZSTQi3xgBF0hsMY2DuFPYeqcYTmI483on8vMtN78PetxTQQYHxOb9bsKcUBYwZmMG5wJlsLKvhoxyEcECQABkOy0vi0sJKxg3qxbm8pDmPa8Cx2FFWRV1LtlcSMIMZZgzI4e0gfnvjnztDrgYmDe5NfXkut9/jAnMxU7xhG357JHCir4bTMVJKTEmgIwoBePVi1u4SSqgYchN/L1z87mlfW72dLfgU5mSnsK61l+vA+vPPJwfDncOfMkXxSUM6ovunsLDpMUkKAGz5zOgcralj+yUEuPas/A3r1YNYv3qW8pvnvxCNfnMhPXt/C/tLq8LIZo7NZtqWQEdnpbC0oZ1h2GtsLK8CLrdG/TRpIWlICu4oPs/yToqj9Ds9O5dPCw7T2P89hpCTGUVl7JKapQzPp2SOBv32cH172zc+OpC7oWLO7hM+N6cdDr26itj6Iw/jm50ayeEUeWWmJjB3Uk+dX5oXOeYSzBqQzZ0oOL63Zx9q8Uq4cP4B5nzmNH76yifV7y0LHHZbJh9uLAbj53CE89c+dANx10QiKKmpZ+NFu4gPGnMmDWLwyD4BLx/ZnzqQc7v3LBjJTE7ls7ADqG4JMPK0XyzYX8swHu/j82H4UlteweldJOJ5/v2gEQedYs6eE97YVcdfFIwiY8eK/9jJ9eBZl1bW8uu4AAOeOyOKcoZn87G+fNDt/nx/bj+HZaYzql85zH+0Ox99oVL907r50NP/3rU/42Hufv/jSJP6x9SD7Sqp4d+tB5n3mNA5W1PDGhiPnOzkhjmtzc3jm/V3hZdNH9OHqSYPYcfAwAzKS+f17u9hWUMElZ/XjzY35XDq2H69H7KPxNbOmjOBzk0a08htwdGa2yjmX2+I6vyX0o/m0sIJ9JVUs3VRAckIc86cPoX9GMjc88SFl1fXcN2sMDUFHcUUNKz7NZ87EfsxdsJwzs5OZNqQne4vKOH9YBo8t3cQlZ/Rmy95iSsorSbB6Eqhn2unpVFVXsyO/hESrI4EG4mkgnnqSLEjA1ZNg3nwgyOxxfXlt7R7iqQ9taw2kxjuC9XXEESRAMOrfODsy7Qhw2CVhOPpYGb2tnHiCxFPv/TkIUkUSgcQeuNpKenI4tNxUyhHp6taePp8JN/+8Q689ZRJ6R5RW1pGeHE8gcKR11RB0xAWM3UWVLN2cz+h+6Uw+vTdJ8QHMjBdW5TFuUAYDeyVT1+CormtgYK8eAPzqnW0Mz07j0rH9AViXV8Khyjre3pTPiH7pzD17MHUNjqq6BlKT4vj1sk+pb3D8Y9tBJg7uxd2Xjub37+3iygkDeH5lHi+syuNX8yYzICOZ3imJxAWMTfvL+Ou6/dx+4TB6pSSyevchBmQkMyCjB299nM+I7FT690ykvLqOv2/J5+whvTi9dw/e21pAgCC/WbaVi0dnMmNkHwZlJFB6uIb8sipG901lfV4Jz36wkyvGDcAIsvdQJReOyiI7LZHqunqqaus5VFHD6t2HmJiTTnlVPaP7pbF6VzGfGdab7YXlBIDeKQkcOlzNz/72CT0SAozITmVrfinnDc9iRN9UeqfEc++f12M47pt1Jj/868d84+LhTBiUwTtb8tlyoIzEQOgb2Q1TT2NoVgo9EoyCsmoe+dsnjOibyrA+PaiuqychYIwd2JNRfVMprKjh0OFalm8tpKSyjoKyGr4wZVDoM7YAb358gD3FlcyZnENVXQOf5Jfjgo7Lxw/AOUdNXZDR/dP5+dKt7Ck+TN/0JArKaxgzoCdjBvZk1a5D9EyOp7y6nuT4AHXBID0S4vjSOafhcLy9qYB3thRwwahs0pMSiI+DsQMzSIoP8O7WQipqGhiYkUx1XT3bCg+z+UA5ALMnDiQ9KZ6EOGPZlgJ6pSSyo/AwpdV1fP3ikewqPkx1bZDzRmSxt6SKmvoGdhVVsrPoMOeNyGJXUSXJCXFMPi2DVbtKKCyvJj4uQHJ8gCFZqfROSSQhLsDbm/P53Fn92V5YwQur8pr9fwiYcdlZ/RjUO4W+6Uks3ZzP0KxUiipq2VpQzjVTcjhYXsvbmwvokRhHTV0DM8/oy8f7y9hyoJw5kwaR3iOB1buKSUuKZ2hWKu9uO0hWahIrdxXTNz2ZC0dl8+Q/dwCQ0SOB0qo6zh3eh7MGZVBSVceqnYc4f2QW5dX1VNc1MCAjmafe20lD0DGkTypnDkjng+1FHKqsY3S/dG6Ydjp7S6rYVVTJ5v1llNfUU1h+5Kq3i8/oy7DsNEqrahndvyeHKmv5tKCC2oYgkwb3Ir+8hoUf7GJYdhoDe/UgAGzcXxbexwWjsqmubeCjnaGW/iVj+jOibyq9UhJZujmfj3YUk52WxGmZKaQmxfP3Two5b2QWSfEBlm4qCP1fqKxjzICejB0U+h26btYVpI2Y1uz8t4cSunQJzrmo2viqXcXU1AWZPiKL0qo6MnoknJQ4Kmrq26xHl1bVUdcQJCst6YTGsaPwMNsPVnDFuAHEx0Vfo1DfEKSyriFc3z0RyqvriAsYFdX1FFbUMLJverj/5ETKO1RJZmoizoX+aLdVPw8GXVSjq6y6judX5nFdbg7pLZyf2vogu4sPM7BXj3B/0dHU1gej3ncw6Nh0oIwz+/ek8Vf2kbe28vmz+jNmYM+o19Y3BJt9do22FVQwOLMH+0qqyUxJJCPl+D9LJXQRkW7iaAndd5ctiohIy5TQRUS6CSV0EZFuQgldRKSbUEIXEekmlNBFRLoJJXQRkW5CCV1EpJvotBuLzKwQ2NXmhi3LAg62uVXnU5yxpThjS3HG1smK83TnXIu31nZaQj8eZraytTuluhLFGVuKM7YUZ2x1hThVchER6SaU0EVEugm/JvTHOzuAdlKcsaU4Y0txxlanx+nLGrqIiDTn1xa6iIg0oYQuItJN+C6hm9mlZrbFzLaZ2Xc6OZYnzazAzDZELMs0s7+Z2Vbv397ecjOzR72415nZ5JMY52Aze8fMPjazjWb29a4Yq5klm9lHZrbWi/M/veVDzexDL54/mlmitzzJm9/mrR9yMuL0jh1nZv8ys7921Ri94+80s/VmtsbMVnrLutTn7h27l5m9YGabzWyTmU3ranGa2WjvPDb+lJnZN7pUnM453/wAccCnwDAgEVgLjOnEeC4AJgMbIpb9FPiON/0d4Cfe9OXAa4ABnwE+PIlxDgAme9PpwCfAmK4Wq3e8NG86AfjQO/5i4Hpv+QLgDm/6fwILvOnrgT+exHP6LeD/AX/15rtcjN4xdwJZTZZ1qc/dO/bvga9604lAr64YZ0S8ccAB4PSuFOdJPQkxOInTgDci5r8LfLeTYxrSJKFvAQZ40wOALd70b4C5LW3XCTH/BfhcV44VSAFWA1MJ3X0X3/R3AHgDmOZNx3vb2UmILQdYClwE/NX7D9ulYoyItaWE3qU+dyAD2NH0vHS1OJvEdgnwz64Wp99KLoOAPRHzed6yrqSfc26/N30A6OdNd4nYva/8kwi1frtcrF4pYw1QAPyN0DeyEudcfQuxhOP01pcCfU5CmI8AdwNBb75PF4yxkQPeNLNVZnabt6yrfe5DgULgKa+M9TszS+2CcUa6HnjOm+4ycfotofuKC/1Z7jLXhZpZGvAn4BvOubLIdV0lVudcg3NuIqFW8DnAGZ0bUTQzmwUUOOdWdXYs7XSec24ycBnwNTO7IHJlF/nc4wmVLn/tnJsEHCZUugjrInEC4PWPXAU833RdZ8fpt4S+FxgcMZ/jLetK8s1sAID3b4G3vFNjN7MEQsl8oXPuxa4cK4BzrgR4h1D5opeZxbcQSzhOb30GUHSCQzsXuMrMdgKLCJVdft7FYgxzzu31/i0A/kzoj2RX+9zzgDzn3Ife/AuEEnxXi7PRZcBq51y+N99l4vRbQl8BjPSuKEgk9LXnpU6OqamXgJu86ZsI1asbl9/o9Xx/BiiN+Jp2QpmZAU8Am5xz/7erxmpm2WbWy5vuQajOv4lQYr+mlTgb478GeNtrIZ0wzrnvOudynHNDCP3+ve2cm9eVYmxkZqlmlt44Tajuu4Eu9rk75w4Ae8xstLfoYuDjrhZnhLkcKbc0xtM14jyZHQkx6oy4nNBVGp8C/7uTY3kO2A/UEWplfIVQfXQpsBV4C8j0tjXgV17c64HckxjneYS+Bq4D1ng/l3e1WIHxwL+8ODcA93nLhwEfAdsIfc1N8pYne/PbvPXDTvLnP4MjV7l0uRi9mNZ6Pxsb/790tc/dO/ZEYKX32S8BenfROFMJfcPKiFjWZeLUrf8iIt2E30ouIiLSCiV0EZFuQgldRKSbUEIXEekmlNBFRLoJJXQRkW5CCV1EpJv4/1Z32TXcJdHxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Model | Test Accuracy = 0.6675 | F1 = 0.6386 |\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "g = g.to(device)\n",
    "\n",
    "node_subjects = phenotypes.iloc[g.ndata['idx'].detach().cpu().numpy()]['Smoking'].reset_index(drop=True) # Get node target labels from meta data\n",
    "node_subjects.name = 'Smoking'\n",
    "\n",
    "GCN_input_shapes = g.ndata['feat'].shape[1]\n",
    "\n",
    "labels = F.one_hot(g.ndata['label'].to(torch.int64))\n",
    "\n",
    "output_metrics = []\n",
    "logits = np.array([])\n",
    "labels_all = np.array([])\n",
    "\n",
    "train_tmp_index , test_index = train_test_split(\n",
    "    node_subjects.index, train_size=0.6, stratify=node_subjects\n",
    "    )\n",
    "train_index , val_index = train_test_split(\n",
    "    train_tmp_index, train_size=0.8, stratify=node_subjects.loc[train_tmp_index]\n",
    "    )\n",
    "\n",
    "model = GCN(GCN_input_shapes , [ 512 ,128 , 64], len(node_subjects.unique())).to(device) \n",
    "print(model)\n",
    "print(g)\n",
    "\n",
    "loss_plot = train(g, g.ndata['feat'] , train_index , val_index , device ,  model , labels , 1000 , 0.0001 , 50)\n",
    "plt.title(f'Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "y_vals = labels.detach().cpu().numpy()\n",
    "\n",
    "test_output_metrics = evaluate(test_index , device , g , g.ndata['feat'] , model , labels )\n",
    "\n",
    "logits = np.vstack((logits , test_output_metrics[5])) if logits.size else test_output_metrics[5] # Concatenate logits from each fold\n",
    "labels_all = np.vstack((labels_all , test_output_metrics[6])) if labels_all.size else test_output_metrics[6] # Concatenate labels from each fold\n",
    "\n",
    "print(\n",
    "    \"GNN Model | Test Accuracy = {:.4f} | F1 = {:.4f} |\".format(\n",
    "     test_output_metrics[1] , test_output_metrics[2] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38418729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression ACC : 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "input_data = g.ndata['feat'].detach().cpu().numpy()\n",
    "\n",
    "glmnet = LogisticRegression(penalty='l1' , solver='saga', max_iter = 1000 )\n",
    "glmnet.fit(input_data[train_index] , y_vals[train_index][: , 0])\n",
    "glmnet_acc = glmnet.score(input_data[test_index] , y_vals[test_index][: , 0])\n",
    "print(f'Baseline Logistic Regression ACC : {glmnet_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "640f8e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Boolean index has wrong length: 112 instead of 23676",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m g \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mfrom_networkx(G , node_attrs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m w1_select \u001b[38;5;241m=\u001b[39m \u001b[43mw1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mglmnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(w1_select\u001b[38;5;241m.\u001b[39miloc[g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1185\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1378\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1021\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1021\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1414\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1210\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getbool_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, axis: AxisInt):\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;66;03m# caller is responsible for ensuring non-None axis\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 1210\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1211\u001b[0m     inds \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2674\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_like(result):\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;66;03m# GH 33924\u001b[39;00m\n\u001b[0;32m   2672\u001b[0m     \u001b[38;5;66;03m# key may contain nan elements, check_array_indexer needs bool array\u001b[39;00m\n\u001b[0;32m   2673\u001b[0m     result \u001b[38;5;241m=\u001b[39m pd_array(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m-> 2674\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_array_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:539\u001b[0m, in \u001b[0;36mcheck_array_indexer\u001b[1;34m(array, indexer)\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# GH26658\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(array):\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolean index has wrong length: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(indexer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         )\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer_dtype(dtype):\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: Boolean index has wrong length: 112 instead of 23676"
     ]
    }
   ],
   "source": [
    "g = dgl.from_networkx(G , node_attrs=['idx' , 'label'])\n",
    "\n",
    "w1_select = w1.loc[ : , (glmnet.coef_ > 0)[0]]\n",
    "\n",
    "g.ndata['feat'] = torch.Tensor(w1_select.iloc[g.ndata['idx'].numpy()].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "g = g.to(device)\n",
    "\n",
    "node_subjects = phenotypes.iloc[g.ndata['idx'].detach().cpu().numpy()]['Smoking'].reset_index(drop=True) # Get node target labels from meta data\n",
    "node_subjects.name = 'Smoking'\n",
    "\n",
    "GCN_input_shapes = g.ndata['feat'].shape[1]\n",
    "\n",
    "labels = F.one_hot(g.ndata['label'].to(torch.int64))\n",
    "\n",
    "output_metrics = []\n",
    "logits = np.array([])\n",
    "labels_all = np.array([])\n",
    "\n",
    "train_tmp_index , test_index = train_test_split(\n",
    "    node_subjects.index, train_size=0.6, stratify=node_subjects\n",
    "    )\n",
    "train_index , val_index = train_test_split(\n",
    "    train_tmp_index, train_size=0.8, stratify=node_subjects.loc[train_tmp_index]\n",
    "    )\n",
    "\n",
    "model = GCN(GCN_input_shapes , [ 64 , 32  , 16], len(node_subjects.unique())).to(device) \n",
    "print(model)\n",
    "print(g)\n",
    "\n",
    "loss_plot = train(g, g.ndata['feat'] , train_index , val_index , device ,  model , labels , 2000 , 0.0001 , 50)\n",
    "plt.title(f'Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "y_vals = labels.detach().cpu().numpy()\n",
    "\n",
    "test_output_metrics = evaluate(test_index , device , g , g.ndata['feat'] , model , labels )\n",
    "\n",
    "logits = np.vstack((logits , test_output_metrics[5])) if logits.size else test_output_metrics[5] # Concatenate logits from each fold\n",
    "labels_all = np.vstack((labels_all , test_output_metrics[6])) if labels_all.size else test_output_metrics[6] # Concatenate labels from each fold\n",
    "\n",
    "print(\n",
    "    \"GNN Model | Test Accuracy = {:.4f} | F1 = {:.4f} |\".format(\n",
    "     test_output_metrics[1] , test_output_metrics[2] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dae07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
